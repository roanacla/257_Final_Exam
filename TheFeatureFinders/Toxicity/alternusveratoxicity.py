# -*- coding: utf-8 -*-
"""AlternusVeraToxicity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g3ZmF6q1G03bQcLCfDr5pU8_cRySlgGY
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, mean_squared_error, mean_absolute_error
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import tree
import pickle
# %matplotlib inline
import re
from gensim.models import Doc2Vec
from sklearn import utils
import gensim
from gensim.models.doc2vec import TaggedDocument
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords  # stop words are, is, the etc. which are not needed for model
from nltk.corpus import words 
from textblob import TextBlob, Word
from nltk.stem.porter import PorterStemmer
from gensim.models import word2vec 
from tqdm import tqdm
from sklearn import feature_extraction
import nltk
import statistics
nltk.download('punkt')
nltk.download('wordnet')
tqdm.pandas(desc="progress-bar")
CV_N_REPEATS = 20
BINS = 10

class ToxicityFeature():
    
    def combine_column(tuple1):
      if(pd.notna(tuple1[1])):
          if(tuple1[1].strip(' \t\n\r') == ''):
              return 'NA'
          else:
              return tuple1[1]
      elif(pd.notna(tuple1[0])):
          if(tuple1[0].strip(' \t\n\r') == ''):
              return 'NA'
          else:
              return tuple1[0]   
      else:
              return 'NA'

    # Corpus cleaning
    def clean_str(self,string):
      STOPWORDS = set(stopwords.words('english'))
      """
      Tokenization/string cleaning for datasets.
      Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
      """
      string = re.sub(r"^b", "", string)
      string = re.sub(r"\\n ", "", string)
      string = re.sub(r"\'s", "", string)
      string = re.sub(r"\'ve", "", string)
      string = re.sub(r"n\'t", "", string)
      string = re.sub(r"\'re", "", string)
      string = re.sub(r"\'d", "", string)
      string = re.sub(r"\'ll", "", string)
      string = re.sub(r",", "", string)
      string = re.sub(r"!", " ! ", string)
      string = re.sub(r"\(", "", string)
      string = re.sub(r"\)", "", string)
      string = re.sub(r"\?", "", string)
      string = re.sub(r"'", "", string)
      string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
      string = re.sub(r"[0-9]\w+|[0-9]","", string)
      string = re.sub(r"\s{2,}", " ", string)
      string = ' '.join(Word(word).lemmatize() for word in string.split() if word not in STOPWORDS) # delete stopwors from text

      return string.strip().lower()

    def label_sentences(self,corpus, label_type):
        """
        Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.
        We do this by using the TaggedDocument method. The format will be "TRAIN_i" or "TEST_i" where "i" is
        a dummy index of the post.
        """
        labeled = []
        for i, v in enumerate(corpus):
            label = label_type + '_' + str(i)
            labeled.append(TaggedDocument(v.split(), [label]))
        return labeled

    def get_vectors(self,model, corpus_size, vectors_size, vectors_type):
        """
        Get vectors from trained doc2vec model
        :param doc2vec_model: Trained Doc2Vec model
        :param corpus_size: Size of the data
        :param vectors_size: Size of the embedding vectors
        :param vectors_type: Training or Testing vectors
        :return: list of vectors
        """
        vectors = np.zeros((corpus_size, vectors_size))
        for i in range(0, corpus_size):
            prefix = vectors_type + '_' + str(i)
            vectors[i] = model.docvecs[prefix]
        return vectors
    
    def FeatureFinders_getToxicityScore(self,headline,body):
                
        #clean
        headline = self.clean_str(headline)
        body = self.clean_str(body)

        all_text = headline+body

        #print(all_text)

        all_text = self.label_sentences(all_text, "Test")

        dbowfile = open('/content/TheFeatureFinders/SupportingFiles/model_dbow', 'rb')
        model_dbow = pickle.load(dbowfile)
        

        #Doc2Vec
        test1_vectors_dbow = self.get_vectors(model_dbow, len(all_text), 300, 'Test')

        #best Model for toxicity prediction
        toxicityModel_file = open('/content/TheFeatureFinders/SupportingFiles/toxicity-model', 'rb')
        best_clf = pickle.load(toxicityModel_file)

        #predictions
        predictedToxicity = best_clf.predict(test1_vectors_dbow)
        predictedToxicity = set(predictedToxicity)
        predictedToxicity = statistics.mean(predictedToxicity)
        #predicedProb = best_clf.predict_proba(test1_vectors_dbow)[:,1] 
        #print(predictedToxicity)

        #best Model for fakenews prediction
        toxicityLabel_file = open('/content/TheFeatureFinders/SupportingFiles/toxicityFakenewsLabel-model', 'rb')
        best_labelclf = pickle.load(toxicityLabel_file)

        predictedFakeNews = best_labelclf.predict(predictedToxicity)
        predicedProb = best_labelclf.predict_proba(predictedToxicity)[:,0] 

        #print(predictedFakeNews)
        #print(predicedProb)
        return float(predicedProb[0])

