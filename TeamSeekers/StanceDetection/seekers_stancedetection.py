# -*- coding: utf-8 -*-
"""Seekers_StanceDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-oZslwrqjC8tJ5Q2ST0wqAmvcgxdJA-L

##**Stance Detection**

Standard imports:
"""

from nltk.stem import WordNetLemmatizer, SnowballStemmer
stemmer = SnowballStemmer('english')
import nltk
nltk.download('wordnet')

import cloudpickle as cp
from urllib.request import urlopen

import string

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

sw = stopwords.words('english')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""Fetching the model from github:   
Random Forest produced best result 
"""

import requests, zipfile, io
def download_url(url, save_path, chunk_size=128):
    r = requests.get(url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    z.extractall(save_path)

download_url('https://github.com/jrangu/Datasets/blob/master/randomforest.zip?raw=true', '/content/')

import pickle

"""Loading the pickled model and predicting the score"""

class Seekers_StanceDetection:
  def remove_stop_and_short_words(self,text):
    text = [word.lower() for word in text.split() if (word.lower() not in sw) and (len(word)>3)]
    return " ".join(text)
  
  def lemmatize_stemming(self,text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
  
  def remove_punctuation(self,text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)
  
  def process_data(self,text):
    # print ('Input Text :: ' + text)
    text = self.remove_stop_and_short_words(text)
    #print('Stopwords and short words removed :: ' + text)
    text = self.lemmatize_stemming(text)
    #print('Lemmatized :: ' + text)
    text = self.remove_punctuation(text)
    #print('Punctuation removed :: ' + text)
    return text

  def predict(self, text):
    loaded_model = pickle.load(open('randomforest.sav', 'rb'))
    #loaded_model = cp.load(urlopen("https://drive.google.com/file/d/1Hs7Ky85bGDsaKZnERXMmQBIXCvRoWoZZ/view?usp=sharing"))
    processedText = self.process_data(text)
    result = loaded_model.predict_proba([processedText])[:,0][0]
    return result