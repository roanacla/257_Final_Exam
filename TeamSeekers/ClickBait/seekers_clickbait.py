# -*- coding: utf-8 -*-
"""Seekers_ClickBait.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8HBJH6af9clyOREuFLlXQdHfsbGAlmh
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_extraction.text import TfidfVectorizer
import cloudpickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from gensim.models.word2vec import Word2Vec
from gensim.models.doc2vec import TaggedDocument
from sklearn.model_selection import train_test_split
import warnings
import numpy as np
np.random.seed(2018)
import nltk
# nltk.download('wordnet')
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import pickle
# nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize 
# nltk.download('punkt')
import nltk
from scipy import sparse
from urllib.request import urlopen
# nltk.download('averaged_perceptron_tagger')
from string import punctuation

class Seekers_ClickBait():

  def tokenization(self,text):
    lst=text.split()
    return lst

  def lowercasing(self,lst):
      new_lst=[]
      for i in lst:
          i=i.lower()
          new_lst.append(i)
      return new_lst
 
  def remove_punctuations(self,lst):
      new_lst=[]
      for i in lst:
          for j in punctuation:
              i=i.replace(j,'')
          new_lst.append(i)
      return new_lst


  def remove_numbers(self,lst):
      nodig_lst=[]
      new_lst=[]
      for i in lst:
          for j in self.digits:    
              i=i.replace(j,'')
          nodig_lst.append(i)
      for i in nodig_lst:
          if i!='':
              new_lst.append(i)
      return new_lst

  def remove_stopwords(self,lst):
      stop=stopwords.words('english')
      new_lst=[]
      for i in lst:
          if i not in stop:
              new_lst.append(i)
      return new_lst
  
  def remove_spaces(self,lst):
    new_lst=[]
    for i in lst:
        i=i.strip()
        new_lst.append(i)
    return new_lst

  
  def lemmatzation(self, lst):
    lemmatizer=nltk.stem.WordNetLemmatizer()
    new_lst=[]
    for i in lst:
        i=lemmatizer.lemmatize(i)
        new_lst.append(i)
    return new_lst

  def predict(self,text):
    multinomial1 = cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/Seekers_ClickBait.sav?raw=true"))
    tfidf =  cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/tfidf.sav?raw=true"))
    dfrme = pd.DataFrame(index=[0], columns=['text'])
    dfrme['text'] = text
    predict=dfrme['text'].apply(self.tokenization)
    predict=predict.apply(self.lowercasing)
    predict=predict.apply(self.remove_punctuations)
    predict=predict.apply(self.remove_stopwords)
    predict=predict.apply(self.remove_spaces)
    predict=predict.apply(self.lemmatzation)
    predict =predict.apply(lambda x: ''.join(i+' ' for i in x))
    text = tfidf.transform(predict)
    train_arr=text.toarray()
    probValue = multinomial1.predict_proba(train_arr)[:,1][0]
    return probValue