# -*- coding: utf-8 -*-
"""Seekers_ClickBait.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8HBJH6af9clyOREuFLlXQdHfsbGAlmh
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.feature_extraction.text import TfidfVectorizer
import cloudpickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from gensim.models.word2vec import Word2Vec
from gensim.models.doc2vec import TaggedDocument
from sklearn.model_selection import train_test_split
import warnings
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import nltk
import pickle
nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize 
nltk.download('punkt')
import nltk
from scipy import sparse
from urllib.request import urlopen
nltk.download('averaged_perceptron_tagger')
from string import punctuation

class Seekers_ClickBait():

  def tokenization(self,text):
    lst=text.split()
    return lst

  def lowercasing(self,lst):
      new_lst=[]
      for i in lst:
          i=i.lower()
          new_lst.append(i)
      return new_lst
 
  def remove_punctuations(self,lst):
      new_lst=[]
      for i in lst:
          for j in punctuation:
              i=i.replace(j,'')
          new_lst.append(i)
      return new_lst


  def remove_numbers(self,lst):
      nodig_lst=[]
      new_lst=[]
      for i in lst:
          for j in self.digits:    
              i=i.replace(j,'')
          nodig_lst.append(i)
      for i in nodig_lst:
          if i!='':
              new_lst.append(i)
      return new_lst

  def remove_stopwords(self,lst):
      stop=stopwords.words('english')
      new_lst=[]
      for i in lst:
          if i not in stop:
              new_lst.append(i)
      return new_lst
  
  def remove_spaces(self,lst):
    new_lst=[]
    for i in lst:
        i=i.strip()
        new_lst.append(i)
    return new_lst

  
  def lemmatzation(self, lst):
    lemmatizer=nltk.stem.WordNetLemmatizer()
    new_lst=[]
    for i in lst:
        i=lemmatizer.lemmatize(i)
        new_lst.append(i)
    return new_lst

  def predict(self,text):
    multinomial1 = cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/Seekers_ClickBait.sav?raw=true"))
    tfidf =  cloudpickle.load(urlopen("https://github.com/Manishayacham/Alternus-Vera/blob/main/tfidf.sav?raw=true"))
    dfrme = pd.DataFrame(index=[0], columns=['text'])
    dfrme['text'] = text
    predict=dfrme['text'].apply(self.tokenization)
    predict=predict.apply(self.lowercasing)
    predict=predict.apply(self.remove_punctuations)
    predict=predict.apply(self.remove_stopwords)
    predict=predict.apply(self.remove_spaces)
    predict=predict.apply(self.lemmatzation)
    predict =predict.apply(lambda x: ''.join(i+' ' for i in x))
    text = tfidf.transform(predict)
    train_arr=text.toarray()
    probValue = multinomial1.predict_proba(train_arr)[:,1][0]
    return probValue