# -*- coding: utf-8 -*-
"""ToxicityClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f2vn2FviqJRxECq6yjDiBAdiydTxu-Pp
"""

# -*- coding: utf-8 -*-
"""Copy of ToxicityClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13g0cwrrcYHD2FxoRNyKw1iT_-NUs140E
"""

class GirlsWhoCode_Toxicity:
  """def encodeLabel(df):
    df.label[df.label == 'FALSE'] = 0
    df.label[df.label == 'half-true'] = 1
    df.label[df.label == 'mostly-true'] = 1
    df.label[df.label == 'TRUE'] = 1
    df.label[df.label == 'barely-true'] = 0
    df.label[df.label == 'pants-fire'] = 0
    return df"""


  def cleaning(raw_news):
    import re
    import nltk
    from nltk.stem.wordnet import WordNetLemmatizer
    # nltk.download('punkt')
    # nltk.download('stopwords')
    # nltk.download('wordnet')
    import nltk

    # 1. Remove non-letters/Special Characters and Punctuations
    news = re.sub("[^a-zA-Z]", " ", raw_news)

    # 2. Convert to lower case.
    news =  news.lower()

    # 3. Tokenize.
    news_words = nltk.word_tokenize( news)

    # 4. Convert the stopwords list to "set" data type.
    stops = set(nltk.corpus.stopwords.words("english"))

    # 5. Remove stop words.
    words = [w for w in  news_words  if not w in stops]

    # 6. Lemmentize
    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]

    # 7. Stemming
    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]

    # 8. Join the stemmed words back into one string separated by space, and return the result.
    return " ".join(stems)

  def getDataFrameWithToxicity(df):

    import tensorflow as tf
    import ktrain
    toxicityPredictor = ktrain.load_predictor('/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar')
    news = df.loc[:,'headline_text']
    dt = news.values
    df['toxicity'] =  toxicityPredictor.predict(dt)
    return df

  def encodeToxicity(df):
    df.toxicity[df.toxicity == 'non-toxic'] = 2
    df.toxicity[df.toxicity == 'toxic'] = 1
    return df

  def getSentiment(df):

    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    sid_obj = SentimentIntensityAnalyzer()
    for i in range(len(df)) :
      sentiment_dict = sid_obj.polarity_scores(df.loc[i,"headline_text"])
      df.loc[i,"POSITIVE"] =sentiment_dict['pos']
      df.loc[i,"NEUTRAL"] =sentiment_dict['neu']
      df.loc[i,"NEGATIVE"] =sentiment_dict['neg']
      df.loc[i,"COMPOUND"] = sentiment_dict['compound']
    for i in range(len(df)) :
      if  df.loc[i,"COMPOUND"] >=0.05:
        sentiment = 'Positive'
      elif df.loc[i,"COMPOUND"] > -0.05 and df.loc[i,"COMPOUND"] < 0.05:
        sentiment = 'Neutral'
      else :
        sentiment = 'Negative'
    df.loc[i,"SENTIMENT"] = sentiment
    df =df.drop(['POSITIVE','NEUTRAL','NEGATIVE','COMPOUND'],axis=1)
    df.SENTIMENT[df.SENTIMENT == 'Positive'] = 3
    df.SENTIMENT[df.SENTIMENT == 'Negative'] = 2
    df.SENTIMENT[df.SENTIMENT == 'Neutral'] = 1
    return df

  def get_word_tokens(text):
    import gensim
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if len(token) > 3:
            result.append(token)
    return result

  def identify_topic_number_score(df,text):
    import gensim
    import pandas as pd
    documents = df[['headline_text']]
    processed_docs = documents['headline_text'].map(GirlsWhoCode_Toxicity.get_word_tokens)
    dictionary = gensim.corpora.Dictionary(processed_docs)
    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)
    bow_vector = dictionary.doc2bow(GirlsWhoCode_Toxicity.get_word_tokens(text))
    topic_number , topic_score = sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1])[0]
    #print (topic_number, topic_score)
    return pd.Series([topic_number, topic_score])


  #Scalar Vector for testing dataset
  def testing_dataset_vector(df_testing):
    vec_testing = []
    for i in range(len(df_testing['toxicity'])):
        vec = df_testing['sentiment_encode'].iloc[i] + df_testing['topic_score'].iloc[i]
        vec_testing.append(vec)
    return vec_testing


  def sentiment_analyzer_scores(df):
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    import math
    analyser = SentimentIntensityAnalyzer()
    sentiment_score = []
    sentiment_labels = {0:'negative', 1:'positive', 2:'neutral'}
    for index,row in df.iterrows():
        score = analyser.polarity_scores(row['headline_text'])
        values = [score['neg'], score['pos'], score['neu']]
        max_index = values.index(max(values))
        data = {'sentiment_score':score, 'sentiment_label':sentiment_labels[max_index], 'sentiment_label_encode': 1+math.log(max_index+1)}
        sentiment_score.append(data)
    return sentiment_score


  def get_toxicity__vector(df):
    from gensim.models.doc2vec import TaggedDocument
    from gensim.models import Doc2Vec
    import nltk
    from nltk import word_tokenize
    # nltk.download('punkt')
    tagged_text = []
    for index, row in df.iterrows():
        tagged_text.append(TaggedDocument(words=word_tokenize(row['headline_text']), tags=[row['label']]))

    tagged_pa_text_test = tagged_text

    #train doc2vec model
    doc2vec_model_pa_test = Doc2Vec(documents = tagged_pa_text_test, dm=0, num_features=500, min_count=2, size=20, window=4)
    y_pa_test, X_pa_test = create_vector_for_learning(doc2vec_model_pa_test, tagged_pa_text_test)
    pa_test = []
    for i in range(len(df['toxicity'])):
        pa_value = df['toxicity'][i]
        pa = doc2vec_model_pa_test[pa_value] + df['SENTIMENT'][i] + df['topic_score'][i]
        pa_test.append(pa)
    return pa_test

  def tag_headline(df, label):
    import gensim
    from gensim.models.doc2vec import TaggedDocument
    import nltk
    from nltk import word_tokenize
    # nltk.download('punkt')
    tagged_text = []
    for index, row in df.iterrows():
        tagged_text.append(TaggedDocument(words=word_tokenize(row['headline_text']), tags=[row[label]]))
    return tagged_text

  def getReadability(df):
    import textstat
    df['ARI'] = df.headline_text.apply(lambda x:textstat.automated_readability_index(x))
    df['DCR'] = df.headline_text.apply(lambda x:textstat.dale_chall_readability_score(x))
    df['TS'] = df.headline_text.apply(lambda x:textstat.text_standard(x,float_output =True))
    return df


  def getToxicityScore(headline):
    #converting the text and the label into a dataframe
    import pandas as pd
    #mounting google drive
    # from google.colab import drive
    # drive.mount('/content/drive/')
    cols = [[headline]]
    df_testing = pd.DataFrame(cols,columns=['headline_text'])
    df_testing.head()

    #encoding the label from text to numeric value
    #df_testing = GirlsWhoCode_Toxicity.encodeLabel(df_testing)
    # print(df_testing.head())

    #cleanup the text
    df_testing['headline_text'] = df_testing["headline_text"].apply(GirlsWhoCode_Toxicity.cleaning)

    #Next to predict the toxicity we will use the BERT model that was trained on the liar liar dataset
    df_testing = GirlsWhoCode_Toxicity.getDataFrameWithToxicity(df_testing)


    #encoding the toxicity
    df_testing = GirlsWhoCode_Toxicity.encodeToxicity(df_testing)
    # print(df_testing.head())
    #applying distillations
    #VADER Sentiment Analysis
    #df_testing = GirlsWhoCode_Toxicity.getSentiment(df_testing)
    sentiment_score_dt = GirlsWhoCode_Toxicity.sentiment_analyzer_scores(df_testing)
    sentiment_score = pd.DataFrame(sentiment_score_dt)

    df_testing['sentiment'] = sentiment_score['sentiment_label']
    df_testing['sentiment_encode'] = sentiment_score['sentiment_label_encode']
    # print(df_testing.head())
    #Topic modelling
    import gensim
    from gensim.models.doc2vec import TaggedDocument

    from gensim.models.doc2vec import TaggedDocument
    from gensim.models import Doc2Vec
    import numpy as np
    df_testing[['topic_number','topic_score']] = df_testing.apply(lambda row: GirlsWhoCode_Toxicity.identify_topic_number_score(df_testing,row['headline_text']), axis=1)
    df_testing = GirlsWhoCode_Toxicity.getReadability(df_testing)
    X_test = np.array(GirlsWhoCode_Toxicity.testing_dataset_vector(df_testing))
    X_test =  X_test.reshape(-1, 1)
    #y_test = df_testing['label']
    import pickle
    fake_news_classifier = pickle.load(open('/content/GirlsWhoCode/SupportingFiles/toxicityModel.sav', 'rb'))
    cols = list(df_testing.columns)
    cols.remove('headline_text')
    #cols.remove('label')
    cols.remove('sentiment')
    predicted = fake_news_classifier.predict(df_testing[cols])
    predicedProb = fake_news_classifier.predict_proba(df_testing[cols])[:,1]

    score = 1 - float(predicedProb)
    return score
