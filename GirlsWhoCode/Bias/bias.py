# -*- coding: utf-8 -*-
"""Bias.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JCKztviIQ2xT_h23kfLw1eloKMSmkFSQ
"""

class Gwc_Bias():

    def cleaning(raw_news):
      import nltk
      import re
      from nltk.stem.wordnet import WordNetLemmatizer
      lemmatizer = nltk.WordNetLemmatizer()
      from nltk.corpus import stopwords
      from string import punctuation
      # nltk.download('punkt')
      # nltk.download('averaged_perceptron_tagger')
      # nltk.download('wordnet')
      # nltk.download('stopwords')

    
      # 1. Remove non-letters/Special Characters and Punctuations
      news = re.sub("[^a-zA-Z]", " ", raw_news)
      
      # 2. Convert to lower case.
      news =  news.lower()
      
      # 3. Tokenize.
      news_words = nltk.word_tokenize(news)
      
      # 4. Convert the stopwords list to "set" data type.
      stops = set(nltk.corpus.stopwords.words("english"))
      
      # 5. Remove stop words. 
      words = [w for w in  news_words  if not w in stops]
      
      # 6. Lemmentize 
      wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]
      
      # 7. Stemming
      #stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]
      
      # 8. Join the stemmed words back into one string separated by space, and return the result.
      return " ".join(wordnet_lem)

    def process_text(text):
      import re
      from string import punctuation
      result = text.replace('/', '').replace('\n', '')
      result = re.sub(r'[1-9]+', 'number', result)
      result = re.sub(r'(\w)(\1{2,})', r'\1', result)
      result = re.sub(r'(?x)\b(?=\w*\d)\w+\s*', '', result)
      result = ''.join(t for t in result if t not in punctuation)
      result = re.sub(r' +', ' ', result).lower().strip()
      return result

    def get_pos(text):
      import nltk
      from nltk import word_tokenize, pos_tag

      tokens = [nltk.word_tokenize(text)]
      postag = [nltk.pos_tag(sent) for sent in tokens][0]
      pos_list = []
      for tag in postag:
        if (tag[1].startswith('NN') or tag[1].startswith('JJ') or tag[1].startswith('VB')):
          pos_list.append(tag[0])
      return pos_list


    def get_intersection(pos, bias_list):
      return len(list(set(pos) & set(bias_list)))
    
    def get_wasserstein_dist(total_spin, total_subj, total_sens, sentiment_score):
      from scipy.stats import wasserstein_distance

      dist_from_bias = wasserstein_distance([1, 2, 0, 0.1531], [total_spin, total_subj, total_sens, sentiment_score])
      dist_from_unbias = wasserstein_distance([0, 0, 0, 0], [total_spin, total_subj, total_sens, sentiment_score])
      dist_btn = abs(dist_from_bias - dist_from_unbias)
      if (dist_from_bias < dist_btn/3):
        return 1  #highly_biased
      elif (dist_from_unbias < dist_btn/3):
        return 3 #least biased
      else:
        return 2

    
    def get_senti(sentence):
      import nltk.sentiment
      # nltk.download('vader_lexicon')
      from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
      senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()

      sentimentVector = []
      snt = senti.polarity_scores(sentence)
      sentimentVector.append(snt['neg'])
      sentimentVector.append(snt['neu'])
      sentimentVector.append(snt['pos'])
      sentimentVector.append(snt['compound'])
      return sentimentVector 

    def get_encoded_label(label):
      if (label == 'false' or label =='barely-true' or label =='pants-fire' or label == 'FALSE'):
        return 0
      elif ( label =='half-true' or label == 'mostly-true' or label == 'TRUE' or label =='true'):
        return 1

    def get_bias_score(text):
      #mounting google drive
      # from google.colab import drive
      # drive.mount('/content/drive/')
      import pandas as pd
      import numpy as np
      import pickle
      from sklearn import metrics
      from sklearn.feature_extraction.text import TfidfVectorizer
      from sklearn.feature_extraction.text import CountVectorizer
      from sklearn.model_selection import train_test_split
      from sklearn.linear_model import LogisticRegression
      from sklearn.metrics import classification_report
      from sklearn.preprocessing import StandardScaler 
      from scipy import sparse
      
        
      #creating the dataframe with our text so we can leverage the existing code
      dfrme = pd.DataFrame(index=[0], columns=['text'])
      dfrme['text'] = text

      #bias dict
      subjective_words = ['good','better', 'best', 'bad', 'worse', 'worst', 'considered', 'dangerous', 'seemingly', 'suggests', 'decrying', 'apparently', 'possibly', 'could', 'would']
      sensationalism_words = ['shocking', 'remarkable', 'rips', 'chaotic', 'lashed', 'onslaught', 'scathing', 'showdown', 'explosive','slams', 'forcing','warning','embroiled','torrent', 'desperate']
      spin_words = ['emerge','serious','refuse','crucial','high-stakes','tirade','landmark','major','critical','decrying','offend','stern','offensive','meaningful','significant','monumental','finally','concede','dodge','latest','admission','acknowledge','mock','rage','brag','lashed','scoff','frustrate','incense','erupt','rant','boast','gloat','fume',]
        
      #Creating some latent variables from the data
      dfrme['clean']     = dfrme['text'].apply(lambda x: Gwc_Bias.cleaning(x))
      dfrme['clean']     = dfrme['clean'].apply(lambda x: Gwc_Bias.process_text(x))
      dfrme['num_words']     = dfrme['clean'].apply(lambda x: len(x.split()))
      dfrme['pos']     = dfrme['clean'].apply(lambda x: Gwc_Bias.get_pos(x))
      dfrme['sentiment_vector'] = dfrme['clean'].apply(lambda x: Gwc_Bias.get_senti(x))
      dfrme['sentiment_score'] = dfrme['sentiment_vector'].apply(lambda x: x[1:][-1])
      dfrme['total_spin_bias']  = dfrme['pos'].apply(lambda x: Gwc_Bias.get_intersection(x, spin_words))
      dfrme['total_subj_bias']  = dfrme['pos'].apply(lambda x: Gwc_Bias.get_intersection(x, subjective_words))
      dfrme['total_sens_bias']  = dfrme['pos'].apply(lambda x: Gwc_Bias.get_intersection(x, sensationalism_words))
      dfrme['total']     = dfrme.apply(lambda x: x.total_spin_bias + x.total_subj_bias + x.total_sens_bias, axis=1)
      dfrme['bias']      = dfrme.apply(lambda x: Gwc_Bias.get_wasserstein_dist(x.total_spin_bias, x.total_subj_bias, x.total_sens_bias, x.sentiment_score), axis =1)

      tfidf = pickle.load(open('/content/GirlsWhoCode/SupportingFiles/tfidf_bias.pk', 'rb'))
      sc = pickle.load(open('/content/GirlsWhoCode/SupportingFiles/scaler_bias.pkl', 'rb'))
      # predicted_LogR = self.model.predict(X_test)
      # score = metrics.accuracy_score(y_test, predicted_LogR)
      Xtxt_text  = tfidf.transform(dfrme['text'])
      Xtxt_bias = dfrme['bias'].values.reshape(-1, 1)
      Xtxt_bias = sc.fit(Xtxt_bias).transform(Xtxt_bias)

      Xtxt  = sparse.hstack([Xtxt_text, Xtxt_bias]).tocsr()

      fakeNewsClassifier = pickle.load(open('/content/GirlsWhoCode/SupportingFiles/Bias_model.sav', 'rb'))
      predicted = fakeNewsClassifier.predict(Xtxt)
      predicedProb = fakeNewsClassifier.predict_proba(Xtxt)[:,1]
      #return predicted, predicedProb
      score = 1 - float(predicedProb)
      return score