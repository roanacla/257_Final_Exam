learning: {early_stopping_patience: 10, log_metrics: auto, stop_metric: auto, valid_ratio: 0.15}
misc: {exp_dir: /content/ag_text/task4, seed: 123}
model:
  backbone: {name: google_electra_small}
  network:
    agg_net:
      activation: tanh
      agg_type: concat
      data_dropout: false
      dropout: 0.1
      initializer:
        bias: [zeros]
        weight: [xavier, uniform, avg, 3.0]
      mid_units: -1
      norm_eps: 1.0e-05
      normalization: layer_norm
      num_layers: 0
    categorical_net:
      activation: leaky
      data_dropout: false
      dropout: 0.1
      emb_units: 32
      initializer:
        bias: [zeros]
        embed: [xavier, gaussian, in, 1.0]
        weight: [xavier, uniform, avg, 3.0]
      mid_units: 64
      norm_eps: 1.0e-05
      normalization: layer_norm
      num_layers: 1
    feature_units: -1
    initializer:
      bias: [zeros]
      weight: [truncnorm, 0, 0.02]
    numerical_net:
      activation: leaky
      data_dropout: false
      dropout: 0.1
      initializer:
        bias: [zeros]
        weight: [xavier, uniform, avg, 3.0]
      input_centering: false
      mid_units: 128
      norm_eps: 1.0e-05
      normalization: layer_norm
      num_layers: 1
    text_net: {pool_type: cls, use_segment_id: true}
  preprocess: {max_length: 128, merge_text: true}
optimization:
  batch_size: 32
  begin_lr: 0.0
  final_lr: 0.0
  layerwise_lr_decay: 0.8
  log_frequency: 0.1
  lr: 4.1195288877731504e-05
  lr_scheduler: triangular
  max_grad_norm: 1.0
  model_average: 5
  num_train_epochs: 4
  optimizer: adamw
  optimizer_params: [('beta1', 0.9), ('beta2', 0.999), ('epsilon', 1e-06), ('correct_bias',
    False)]
  per_device_batch_size: 16
  val_batch_size_mult: 2
  valid_frequency: 0.1
  warmup_portion: 0.1
  wd: 0.01
version: 1
