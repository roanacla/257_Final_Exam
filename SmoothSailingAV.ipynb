{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmoothSailingAV.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMQ27q/U/JJL0WxuU3LFvCR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/257_Final_Exam/blob/main/SmoothSailingAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxn0Z5m127G"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI9u-xyA-7L"
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETwjdcu3rDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ed244a-2085-4faa-c09c-62e5b7ea10ae"
      },
      "source": [
        "#UNSINTALL\n",
        "#Feature finders\n",
        "!pip uninstall -y xgboost\n",
        "clear_output()\n",
        "print('Success Uninstalling libraries')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_FTsPY_bfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cec198-4a46-4453-ad50-bb3661c6d8f8"
      },
      "source": [
        "%rm -r *\n",
        "!git clone https://@github.com/roanacla/257_Final_Exam.git\n",
        "%mv 257_Final_Exam/* /content/\n",
        "%rm -r 257_Final_Exam"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '257_Final_Exam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 338 (delta 33), reused 84 (delta 22), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (338/338), 95.45 MiB | 31.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Checking out files: 100% (118/118), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVyOfyY3ov_",
        "outputId": "5981ab0e-df99-4a28-a7e6-7e125afaf25a"
      },
      "source": [
        "#WGET\n",
        "\n",
        "#Team Seakers\n",
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
        "%mkdir /content/TeamSeekers/SupportingFiles\n",
        "%mv bertmodel /content/TeamSeekers/SupportingFiles/\n",
        "!unzip /content/TeamSeekers/SupportingFiles/bertmodel -d /content/TeamSeekers/SupportingFiles/\n",
        "%rm /content/TeamSeekers/SupportingFiles/bertmodel\n",
        "\n",
        "#FEATURE FINDERS\n",
        "!wget --directory-prefix=/content/TheFeatureFinders/SupportingFiles/ https://mirrors.aliyun.com/pypi/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl#sha256=898f26bb66589c644d17deff1b03961504f7ad79296ed434d0d7a5e9cb4deae6\n",
        "\n",
        "clear_output()\n",
        "print('Success Downloading files')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Downloading files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5itgK72D2GDh",
        "outputId": "6874a03b-ef44-4a6e-8c35-e295166348c0"
      },
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Blastoff\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "#Sigma\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt-get install protobuf-compiler\n",
        "\n",
        "#Cereal Killers\n",
        "!apt-get install git-lfs\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "clear_output()\n",
        "print('Success Installing Libraries')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Installing Libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a7blI8t4f7c"
      },
      "source": [
        "#RESTART RUNTIME - THIS IS A REQUIREMENT BEFORE CONTINUING\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn3j9U816Bv"
      },
      "source": [
        "# Download Large Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEZFScB0osBe"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from IPython.display import Image, clear_output\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAd_rjrAbTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41c6ab7-7e07-4790-b1a1-764b7d2f3d40"
      },
      "source": [
        "#GirlsWhoCode - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='11g4TSsDJ0ojvkua-BBPd7OVPk3o0QEL1',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.preproc',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1GsgExCNM9pCnAB_gAKPxrwDmg03LZhT-',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.h5',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='1RszezV7kgo1Abbh5tgW9MlwW6oq1D8Ue',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/model_dbow',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "#Feature Finders - Title vs Body\n",
        "gdd.download_file_from_google_drive(file_id='19lFvSh2DIs9QAKQ9JbjySH4psWutny_a',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/GoogleNews-vectors-negative300.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#Cereal Killers - Social Credibility, Psychological Utility, BERT \n",
        "gdd.download_file_from_google_drive(file_id='1GGdgYa-V71mkTglwC9OxL0xhPU9p1hjt',\n",
        "                                  dest_path='/content/CerealKillers/BERT/saved_model/pytorch_model.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1vT5vue0V_3K1RmYuDMEkhsfrH413SKjv',\n",
        "                                  dest_path='/content/CerealKillers/PsychologicalUtility/saved_dir/net.params',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "gdd.download_file_from_google_drive(file_id='1yR8bEhdPXlJdw6VLekQHWOmoDpGQre3b',\n",
        "                                  dest_path='/content/TheShinningUnicorns/SupportingFiles/Copy_of_GoogleNews-vectors-negative300.bin.gz',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "clear_output()\n",
        "print('Success clonning repos and downloding files')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success clonning repos and downloding files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8WcHhZe6sm",
        "outputId": "634d69d4-fa95-479c-d384-a9c240297350"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "#Blastoff\n",
        "sys.path.append('/content/Blastoff/ContentStatistics')\n",
        "sys.path.append('/content/Blastoff/ContextVeracity')\n",
        "sys.path.append('/content/Blastoff/Sensationalism')\n",
        "sys.path.append('/content/Blastoff/SupportingFiles')\n",
        "\n",
        "#Cereal Killers\n",
        "sys.path.append('/content/CerealKillers/BERT')\n",
        "sys.path.append('/content/CerealKillers/PsychologicalUtility')\n",
        "sys.path.append('/content/CerealKillers/SocialCredibility')\n",
        "\n",
        "#GirlsWhoCode\n",
        "sys.path.append('/content/GirlsWhoCode/Bias')\n",
        "sys.path.append('/content/GirlsWhoCode/PoliticalAfiliation')\n",
        "sys.path.append('/content/GirlsWhoCode/Topics')\n",
        "sys.path.append('/content/GirlsWhoCode/Toxicity')\n",
        "\n",
        "#GoML\n",
        "sys.path.append('/content/GoML/ClickBait')\n",
        "sys.path.append('/content/GoML/NewsCoverage')\n",
        "sys.path.append('/content/GoML/StanceDetection')\n",
        "sys.path.append('/content/GoML/WritingStyle')\n",
        "sys.path.append('/content/GoML/SupportingFiles')\n",
        "\n",
        "#Sigma\n",
        "sys.path.append('/content/Sigma/Credibility')\n",
        "sys.path.append('/content/Sigma/MaliciousAccount')\n",
        "sys.path.append('/content/Sigma/NetworkBasedPredictor')\n",
        "sys.path.append('/content/Sigma/VerifiableAuthenticity')\n",
        "\n",
        "#Team Seekers\n",
        "sys.path.append('/content/TeamSeekers/BERTMCC')\n",
        "sys.path.append('/content/TeamSeekers/ClickBait')\n",
        "sys.path.append('/content/TeamSeekers/Spam')\n",
        "sys.path.append('/content/TeamSeekers/StanceDetection')\n",
        "\n",
        "#TheFeatureFinders\n",
        "sys.path.append('/content/TheFeatureFinders/ReliableSource')\n",
        "sys.path.append('/content/TheFeatureFinders/StanceDetection')\n",
        "sys.path.append('/content/TheFeatureFinders/Toxicity')\n",
        "sys.path.append('/content/TheFeatureFinders/SupportingFiles')\n",
        "\n",
        "#TheShinningUnicorns\n",
        "sys.path.append('/content/TheShinningUnicorns/SupportingFiles')\n",
        "\n",
        "#Trail Blaizers\n",
        "sys.path.append('/content/Trailblazers/MisleadingIntentions')\n",
        "sys.path.append('/content/Trailblazers/Education')\n",
        "sys.path.append('/content/Trailblazers/EventDetection')\n",
        "\n",
        "print('Success adding paths to the system')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success adding paths to the system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yc9Q96XNZiy",
        "outputId": "7accaa2a-a814-43fa-92d4-2533b2342577"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "import en_core_web_md\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True\n",
        "\n",
        "clear_output()\n",
        "print('Success importing dependencies')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success importing dependencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHL9hWcX17qr"
      },
      "source": [
        "#Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "033XN2S9Gd84",
        "outputId": "c8740424-3972-45e2-a9a2-4bba897c7062"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "print('Finish instantiating #Blastoff - Content Statistics')\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "from sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "print('Finish instantiating #Blastoff - Sensationalism')\n",
        "\n",
        "# #Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "print('Finish instantiating #Blastoff - Context Veracity')\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "print('Finish instantiating #GirlsWhoCode - Toxicity')\n",
        "\n",
        "#GirlsWhoCode - Topics\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "print('Finish instantiating #GirlsWhoCode - Topics')\n",
        "\n",
        "#GirlsWhoCode - Bias\n",
        "from bias import Gwc_Bias\n",
        "print('Finish instantiating #GirlsWhoCode - Bias')\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation\n",
        "print('Finish instantiating #GirlsWhoCode - Political Afiliation')\n",
        "\n",
        "# #Feature Finders - Stance Detection\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()\n",
        "print('Finish instantiating #Feature Finders - Stance Detection')\n",
        "\n",
        "# #Feature Finders - Toxicity\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()\n",
        "print('Finish instantiating #Feature Finders - Toxicity')\n",
        "\n",
        "# #Feature Finders - Reliable Source\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()\n",
        "print('Finish instantiating #Feature Finders - Reliable Source')\n",
        "\n",
        "# #Feature Finders - Title vs Body\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "tvb = titlevsbody.TitleVsBody()\n",
        "print('Finish instantiating #Feature Finders - Title vs Body')\n",
        "\n",
        "#Team Seekers - BERT\n",
        "from seekers_bertmcc import Seekers_BertMcc\n",
        "print('Finish instantiating #Team Seekers - BERT')\n",
        "\n",
        "# #Team Seekers - Stance Detection\n",
        "from seekers_stancedetection import Seekers_StanceDetection\n",
        "print('Finish instantiating #Team Seekers - Stance Detection')\n",
        "\n",
        "# #Team Seekers - ClickBait\n",
        "from seekers_clickbait import Seekers_ClickBait\n",
        "print('Finish instantiating #Team Seekers - ClickBait')\n",
        "\n",
        "# #Team Seekers - Spam\n",
        "from seekers_spam import Seekers_Spam\n",
        "print('Finish instantiating #Team Seekers - Spam')\n",
        "\n",
        "# #TrailBlazers - Misleading Intentions\n",
        "import alternusvera_misleading_intentions as MI\n",
        "print('Finish instantiating #TrailBlazers - Misleading Intentions')\n",
        "\n",
        "# #TrailBlazers - Education\n",
        "import alternusvera_education as AVE\n",
        "print('Finish instantiating #TrailBlazers - Education')\n",
        "\n",
        "# #TrailBlazers - Event Coverage\n",
        "import EventCoverage as EC\n",
        "print('Finish instantiating #TrailBlazers - Event Coverage')\n",
        "\n",
        "# #GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "# from go_ml import *\n",
        "# print('Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style')\n",
        "\n",
        "#Sigma - MaliciousAccount\n",
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/Sigma/MaliciousAccount/malicious_account_MLP.pkl\")\n",
        "print('Finish instantiating #Sigma - MaliciousAccount')\n",
        "\n",
        "# #Sigma - Credibility\n",
        "from SIGMAcredibility  import Credibility\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/Sigma/Credibility/credibility_model.pkl\")\n",
        "print('Finish instantiating #Sigma - Credibility')\n",
        "\n",
        "# #Sigma Network Based Predictor\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/Sigma/NetworkBasedPredictor/networkbased.pkl')\n",
        "print('Finish instantiating #Sigma Network Based Predictor')\n",
        "\n",
        "# #Sigma Verifiable Authenticity\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "va = VerifiableAuthenticity()\n",
        "print('Finish instantiating #Sigma Verifiable Authenticity')\n",
        "\n",
        "# #Ceral Killers - Social Credibility, Psychological Utility, BERT\n",
        "import socialcredibility as SC \n",
        "# import psychologicalutility as PU\n",
        "import bert as BT\n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "# pu = PU.CerealKillers_PsychologicalUtility()\n",
        "bt = BT.CerealKillers_SentimentClassifier()\n",
        "print('Finish instantiating #Ceral Killers - Social Credibility, Psychological Utility, BERT')\n",
        "\n",
        "# #The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "import finalteamintegration as EM\n",
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()\n",
        "print('Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics')\n",
        "\n",
        "# # clear_output()\n",
        "# # print('Success Instantiating Factors')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loading default pretrained model.\n",
            "Downloading 1oFjoL9LWrp2-YPSJL2UhBQ1efV9LiC2n into /content/Blastoff/SupportingFiles/content_statistic_model.pickle... Done.\n",
            "Finish instantiating #Blastoff - Content Statistics\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 1SpfmiCq2a2aXTXvFW6cHnm-0eBCpcyxY into /content/Blastoff/SupportingFiles/sensationalism_BERT_best.model... Done.\n",
            "Finish instantiating #Blastoff - Sensationalism\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Finish instantiating #Blastoff - Context Veracity\n",
            "Finish instantiating #GirlsWhoCode - Toxicity\n",
            "Finish instantiating #GirlsWhoCode - Topics\n",
            "Finish instantiating #GirlsWhoCode - Bias\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #GirlsWhoCode - Political Afiliation\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Stance Detection\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Toxicity\n",
            "Finish instantiating #Feature Finders - Reliable Source\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Title vs Body\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #Team Seekers - BERT\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #Team Seekers - Stance Detection\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Finish instantiating #Team Seekers - ClickBait\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Finish instantiating #Team Seekers - Spam\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Misleading Intentions\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Education\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Event Coverage\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Finish instantiating #Sigma - MaliciousAccount\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Finish instantiating #Sigma - Credibility\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Finish instantiating #Sigma Network Based Predictor\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "Finish instantiating #Sigma Verifiable Authenticity\n",
            "Finish instantiating #Ceral Killers - Social Credibility, Psychological Utility, BERT\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1FXU_sTeb5"
      },
      "source": [
        "# Before Poli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5u_4iKQTddb"
      },
      "source": [
        "#Feature finders\n",
        "\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]\n",
        "\n",
        "# Trail Blaizers\n",
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcsZksxrHlFI"
      },
      "source": [
        "def getProbablityScore(category):\n",
        "\n",
        "    if(category =='pants-fire' ):\n",
        "        return 0.9;\n",
        "    elif(category =='false'):\n",
        "        return 0.73;\n",
        "    elif(category =='full-flop' or category == 'barely-true'):\n",
        "        return 0.56;\n",
        "    elif(category =='half-true' or category == 'half-flip'):\n",
        "        return 0.4;\n",
        "    elif(category =='mostly-true'):\n",
        "        return 0.24;\n",
        "    else:\n",
        "        return 0.1;\n",
        "\n",
        "\n",
        "def isFakeNews(headline_text, body, partyaffiliation,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    headline_text = headline_text\n",
        "    #label_encode = label_encode\n",
        "    accur = [0.57, 0.56, 0.58, 0.56,\n",
        "             0.96, 0.64, 0.76, 0.47, \n",
        "             0.64, 0.56, 0.84, \n",
        "             0.87, 0.70, 0.82, 0.85,\n",
        "             0.78, 0.82, 0.48, 0.72,\n",
        "             0.75, 0.77, 0.77, 0.70,\n",
        "             0.26, 0.28, 0.49,\n",
        "             0.82, 0.35, 0.55,\n",
        "             0.58, 0.54] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    print(w)\n",
        "\n",
        "    print(\"here\")\n",
        "    #For Toxicity - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score1 = GirlsWhoCode_Toxicity.getToxicityScore(headline_text)\n",
        "        print(\"probability of news not being fake from the toxicity model = \",score1)\n",
        "        prob.append(w[0] * score1)\n",
        "        sumW += w[0]\n",
        "\n",
        "    # #For Topic Modelling for LDA - GWC\n",
        "    # if headline_text != \"\" :\n",
        "    #     score2 = Topics_with_LDA_Bigram.getTopicScoreBigramLDAModel(headline_text)\n",
        "    #     prob.append(w[1] * score2)\n",
        "    #     print(\"probability of news not being fake from the topic model = \",score2)\n",
        "    #     sumW += w[1] \n",
        "\n",
        "    # #For Bias - GWC\n",
        "    # if headline_text != \"\":\n",
        "    #     score3 = Gwc_Bias.get_bias_score(headline_text)\n",
        "    #     print(\"probability of news not being fake from the bias model = \", score3)\n",
        "    #     prob.append(w[2] * score3)\n",
        "    #     sumW += w[2]\n",
        "\n",
        "    # #For political_affiliaiton - GWC\n",
        "    # if headline_text != \"\" and partyaffiliation != \"\":\n",
        "    #     score4 = Girlswhocode_PoliticalAfiiliation.DATAMINERS_getPartyAffiliationScore(headline_text,partyaffiliation)\n",
        "    #     print(\"probability of news not being fake political affilaiiton model = \", score4)\n",
        "    #     prob.append(w[3] * score4)\n",
        "    #     sumW += w[3]\n",
        "\n",
        "    # #For Clickbait - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "    #     prob.append(w[4] * Seekers_ClickBait().predict(headline_text))\n",
        "    #     sumW += w[4]\n",
        "    \n",
        "    # #For Stance_detection - Seekers\n",
        "    # if (headline_text != \"\"):\n",
        "\n",
        "    #     prob.append(w[5] * Seekers_StanceDetection().predict(headline_text))\n",
        "    #     sumW += w[5]\n",
        "    #For Spam - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "\n",
        "    #     prob.append(w[6] * Seekers_Spam().predict(headline_text))\n",
        "    #     sumW += w[6]\n",
        "    \n",
        "    #For Bert - Seekers\n",
        "    # if (headline_text!=\"\"):\n",
        "\n",
        "    #     prediction, probability = Seekers_BertMcc.get_bert_predictions(headline_text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "    #     prob.append(w[7] * probability)\n",
        "    #     sumW += w[7]\n",
        "\n",
        "    #For Education - Trailblazers\n",
        "    # if (headline_text != \"\"):\n",
        "        # edu_mod_res = AVE.predictLable(headline_text)\n",
        "        # edu_score=getProbablityScore(edu_mod_res)\n",
        "        # print(\"Probability of news baded on education model is \",edu_score)\n",
        "        # prob.append(w[8]* edu_score)\n",
        "        # sumW+=w[8]\n",
        "\n",
        "    #For Event Coverage - Trailblazers\n",
        "        # event_score = EC.predictLable(headline_text)\n",
        "        # print(\"Probability of news baded on Event coverage model is \",event_score[0])\n",
        "        # prob.append(w[9]* event_score[0])\n",
        "        # sumW+=w[9]\n",
        "\n",
        "    #For Misleding intentions - Trailblazers\n",
        "        # mislead_intent = (MI.predictIntention(headline_text))\n",
        "        # mislead_score=getProbablityScore(mislead_intent)\n",
        "        # print(\"Probability of news baded on misleading intension model is \",mislead_score)\n",
        "        # prob.append(w[10]* mislead_score)\n",
        "        # sumW+=w[10]\n",
        "    \n",
        "    # Stance Detection - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[11] * getStanceDetectionScore(body))\n",
        "    #     sumW += w[11]\n",
        "    \n",
        "    # Clickbait - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[12] * getClickbaitScore(body))\n",
        "    #     sumW += w[12]\n",
        "    \n",
        "    # News Coverage - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[13] * getNewsCoverageScore(body))\n",
        "    #     sumW += w[13]\n",
        "    \n",
        "    # Writing Style - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[14] * WritingStyleScore(body))\n",
        "    #     sumW += w[14]\n",
        "\n",
        "    # if (headline_text != \"\"):\n",
        "    # malicious account - Sigma\n",
        "        # factorMA, _, _  = maObject.predict(malicious_accountCls, body, nlp)\n",
        "        # prob.append(w[15] * factorMA ) \n",
        "        # sumW += w[15]\n",
        "        \n",
        "      # credebility - Sigma\n",
        "        # _ , factorCR = credObject.predict(credCls, body , nlp)\n",
        "        # prob.append(w[16] * factorCR ) \n",
        "        # sumW += w[16]\n",
        "\n",
        "      # Network base - Sigma\n",
        "        # factorNB= nb.predict(body, nlp)\n",
        "        # prob.append(w[17] * factorNB ) \n",
        "        # sumW += w[17]\n",
        "\n",
        "      # Authenticity - Sigma\n",
        "        # venue = 'CNN'\n",
        "        # factorVA = va.predict(body, venue)\n",
        "        # prob.append(w[18] * factorVA ) \n",
        "        # sumW += w[18]\n",
        "\n",
        "       # Stance Detection\n",
        "        # prob.append(w[19] * predictStance.FeatureFinders_getStanceScore(headline_text,body))\n",
        "        # sumW += w[19]\n",
        "\n",
        "      # Reliable Source\n",
        "        # prob.append(w[20] * reliablesource.FeatureFinders_getReliabilityBySource(source))\n",
        "        # sumW += w[20]\n",
        "\n",
        "       # Toxicity\n",
        "        # prob.append(w[21] * predictToxicity.FeatureFinders_getToxicityScore(headline_text,body))\n",
        "        # sumW += w[21]\n",
        "      \n",
        "       # Title vs Body\n",
        "        # tvb_value = tvb.FeatureFinders_getTitleVsBodyRelationship(head_line=headline_text, body_text=body)[0]\n",
        "        # prob.append(w[22] * tvb.FeatureFinders_getTitleVsBodyScore(tvb_value)[0][1]) # 0 is Fake, 1 is True\n",
        "        # sumW += w[22]\n",
        "\n",
        "    # Sentiment Classification\n",
        "    # if headline_text != \"\":\n",
        "    #     bt_score = bt.predict(headline_text)\n",
        "    #     prob.append(w[26]* bt_score)\n",
        "    #     sumW+=w[26]\n",
        "\n",
        "    # # # Psychological Utility\n",
        "    # if body != \"\":\n",
        "    #     pu_score = pu.predict(body,barely_true_count,false_count,half_true_count,mostly_true_count,pants_on_fire_count)\n",
        "    #     prob.append(w[27]* pu_score)\n",
        "    #     sumW+=w[27]\n",
        "      \n",
        "    # Social Credibility\n",
        "    # if source != \"\":\n",
        "    #     sc.search_user_by_name(source)\n",
        "    #     sc_score = sc.predict(sc.get_user_data())\n",
        "    #     prob.append(w[28]* sc_score)\n",
        "    #     sumW+=w[28]\n",
        "\n",
        "    # # Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #     in_df = pd.DataFrame(data=[headline_text], columns=['Statement'])\n",
        "    #     res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "    #     prob.append(w[23] * float(res_cs.iloc[0]))\n",
        "    #     sumW+=w[23]\n",
        "\n",
        "    # Context Veracity\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[24]* float(cv.get_veracity_scores(headline_text)))\n",
        "    #     sumW+=w[24]\n",
        "\n",
        "    # # Sensationalism\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[25] * 0.75 * sensa.getScore(headline_text))\n",
        "    #     sumW+=w[25]\n",
        "\n",
        "    # #For Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #      content_Statistics_score = contentStatistics1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on content Statistics model is \",content_Statistics_score)\n",
        "    #      prob.append(w[29]* content_Statistics_score)\n",
        "    #      sumW+=w[29]\n",
        "     #For Echo Chamber\n",
        "    # if (headline_text != \"\"):\n",
        "    #      echo_ChamberMaster_score = echoChamberMaster1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on echo Chamber model is \",echo_ChamberMaster_score)\n",
        "    #      prob.append(w[30]* echo_ChamberMaster_score)\n",
        "    #      sumW+=w[30]\n",
        "\n",
        "    \n",
        "   \n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpY3DW10H1y3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cad2402-d449-421f-ac2e-5a772ca90dc3"
      },
      "source": [
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes','Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.', 'democrat', 'fox news','in a report', 0,50,50,0,0)\n",
        "\n",
        "if(result>=0.8):\n",
        "    print(\"pants-on-fire\")\n",
        "elif(result<0.80 and result>=0.64):\n",
        "    print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "    print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "    print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "    print('mostly-true')\n",
        "else:\n",
        "    print('true')\n",
        "\n",
        "#if result >= 0.5:\n",
        "#     print(\"is FAKE NEWS!!!\")\n",
        "# else:\n",
        "#     print(\"it is NOT fake news!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02844311377245509, 0.02794411177644711, 0.028942115768463072, 0.02794411177644711, 0.04790419161676647, 0.03193612774451098, 0.03792415169660679, 0.023453093812375248, 0.03193612774451098, 0.02794411177644711, 0.041916167664670656, 0.04341317365269461, 0.03493013972055888, 0.04091816367265469, 0.042415169660678646, 0.038922155688622756, 0.04091816367265469, 0.023952095808383235, 0.03592814371257485, 0.037425149700598806, 0.03842315369261477, 0.03842315369261477, 0.03493013972055888, 0.012974051896207586, 0.013972055888223554, 0.02445109780439122, 0.04091816367265469, 0.01746506986027944, 0.027445109780439125, 0.028942115768463072, 0.026946107784431142]\n",
            "here\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading projection weights from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "John McCain\n",
            "Probability of news baded on education model is  0.73\n",
            "Probability of news baded on Event coverage model is  0.39483999772424655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loaded (3000000, 300) matrix from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(9 unique tokens: ['biden', 'cabinet', 'crazy', 'describ', 'fawn']...) from 1 documents (total 9 corpus positions)\n",
            "collecting document frequencies\n",
            "PROGRESS: processing document #0\n",
            "calculating IDF weights for 1 documents and 8 features (9 matrix non-zeros)\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 4000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 2 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #4 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"superhero\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"fawn\"\n",
            "topic #5 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"fawn\"\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #8 (0.100): 0.111*\"cabinet\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"describ\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"rollout\" + 0.111*\"crazy\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic diff=6.262588, rho=1.000000\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #2 (0.100): 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"cabinet\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic #0 (0.100): 0.111*\"cabinet\" + 0.111*\"rollout\" + 0.111*\"describ\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"medium\" + 0.111*\"crazy\"\n",
            "topic #6 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"crazy\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rescue\"\n",
            "topic diff=0.000186, rho=0.707018\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 8000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 4 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #6 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #1 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=6.938342, rho=1.000000\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #7 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #0 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=0.000000, rho=0.707018\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Probability of news baded on misleading intension model is  0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0e08578f6742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misFakeNews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'democrat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fox news'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in a report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pants-on-fire\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.80\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0e2c5e8ab3d6>\u001b[0m in \u001b[0;36misFakeNews\u001b[0;34m(headline_text, body, partyaffiliation, source, context, mostly_true_count, half_true_count, barely_true_count, false_count, pants_on_fire_count)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# #For Content Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m          \u001b[0mcontent_Statistics_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontentStatistics1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m          \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability of news baded on content Statistics model is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m          \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/finalteamintegration.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfTVRNjS195_"
      },
      "source": [
        "#Remove content"
      ]
    }
  ]
}