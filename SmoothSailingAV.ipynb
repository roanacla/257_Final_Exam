{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmoothSailingAV.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPO1woA55e6MCWzFOa2HmH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/257_Final_Exam/blob/main/SmoothSailingAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxn0Z5m127G"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okQD-0UbcHy3"
      },
      "source": [
        "# REMOVE DEFAULT FOLDER\n",
        "%rm sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI9u-xyA-7L"
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETwjdcu3rDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779a3bf8-bd67-42b9-abbe-693b41573331"
      },
      "source": [
        "# UNSINTALL UNNECESARY DEPENDENCIES\n",
        "\n",
        "#Feature finders needs a specific version of xgboost\n",
        "!pip uninstall -y xgboost\n",
        "clear_output()\n",
        "print('Success Uninstalling libraries')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_FTsPY_bfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c07040-7880-4614-fac7-787fde6d1e8e"
      },
      "source": [
        "# CLONE GITHUB REPOSITORY\n",
        "\n",
        "!git clone https://@github.com/roanacla/257_Final_Exam.git\n",
        "%mv 257_Final_Exam/* /content/\n",
        "%rm -r 257_Final_Exam\n",
        "\n",
        "clear_output()\n",
        "print('Success Clonning Repository')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVyOfyY3ov_",
        "outputId": "8ff11737-d93d-4368-f1a0-a665959499d5"
      },
      "source": [
        "# WGET DRIVERS\n",
        "\n",
        "#Feature finders\n",
        "!wget --directory-prefix=/content/TheFeatureFinders/SupportingFiles/ https://mirrors.aliyun.com/pypi/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl#sha256=898f26bb66589c644d17deff1b03961504f7ad79296ed434d0d7a5e9cb4deae6\n",
        "\n",
        "clear_output()\n",
        "print('Success WGET drivers')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Downloading files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5itgK72D2GDh",
        "outputId": "c75faf72-b276-417c-d9cc-ec8fb169dccb"
      },
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Blastoff\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "#Sigma\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt-get install protobuf-compiler\n",
        "\n",
        "#Cereal Killers\n",
        "!apt-get install git-lfs\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "clear_output()\n",
        "print('Success Installing Libraries')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Installing Libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn3j9U816Bv"
      },
      "source": [
        "# Download Large Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEZFScB0osBe"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from IPython.display import Image, clear_output\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAd_rjrAbTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cadf3bb-3dd2-4d9a-aa38-eddde1b64483"
      },
      "source": [
        "#Team Seakers\n",
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
        "%mkdir /content/TeamSeekers/SupportingFiles\n",
        "%mv bertmodel /content/TeamSeekers/SupportingFiles/\n",
        "!unzip /content/TeamSeekers/SupportingFiles/bertmodel -d /content/TeamSeekers/SupportingFiles/\n",
        "%rm /content/TeamSeekers/SupportingFiles/bertmodel\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='11g4TSsDJ0ojvkua-BBPd7OVPk3o0QEL1',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.preproc',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1GsgExCNM9pCnAB_gAKPxrwDmg03LZhT-',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.h5',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='1RszezV7kgo1Abbh5tgW9MlwW6oq1D8Ue',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/model_dbow',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "#Feature Finders - Title vs Body\n",
        "gdd.download_file_from_google_drive(file_id='19lFvSh2DIs9QAKQ9JbjySH4psWutny_a',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/GoogleNews-vectors-negative300.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#Cereal Killers - Social Credibility, Psychological Utility, BERT \n",
        "gdd.download_file_from_google_drive(file_id='1GGdgYa-V71mkTglwC9OxL0xhPU9p1hjt',\n",
        "                                  dest_path='/content/CerealKillers/BERT/saved_model/pytorch_model.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1vT5vue0V_3K1RmYuDMEkhsfrH413SKjv',\n",
        "                                  dest_path='/content/CerealKillers/PsychologicalUtility/saved_dir/net.params',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "gdd.download_file_from_google_drive(file_id='1yR8bEhdPXlJdw6VLekQHWOmoDpGQre3b',\n",
        "                                  dest_path='/content/TheShinningUnicorns/SupportingFiles/Copy_of_GoogleNews-vectors-negative300.bin.gz',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "clear_output()\n",
        "print('Success clonning repos and downloding files')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success clonning repos and downloding files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSmkGQ9pd5y8"
      },
      "source": [
        "## IMPORTANT - PLEASE RESTART RUNTIME\n",
        "\n",
        "Some libraries need the runtime to restart. Thanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhk3XTRFeMp7"
      },
      "source": [
        "# Set System Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8WcHhZe6sm",
        "outputId": "2bc01370-d1c4-48e5-b1e8-5ef70beee092"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "#Blastoff\n",
        "sys.path.append('/content/Blastoff/ContentStatistics')\n",
        "sys.path.append('/content/Blastoff/ContextVeracity')\n",
        "sys.path.append('/content/Blastoff/Sensationalism')\n",
        "sys.path.append('/content/Blastoff/SupportingFiles')\n",
        "\n",
        "#Cereal Killers\n",
        "sys.path.append('/content/CerealKillers/BERT')\n",
        "sys.path.append('/content/CerealKillers/PsychologicalUtility')\n",
        "sys.path.append('/content/CerealKillers/SocialCredibility')\n",
        "\n",
        "#GirlsWhoCode\n",
        "sys.path.append('/content/GirlsWhoCode/Bias')\n",
        "sys.path.append('/content/GirlsWhoCode/PoliticalAfiliation')\n",
        "sys.path.append('/content/GirlsWhoCode/Topics')\n",
        "sys.path.append('/content/GirlsWhoCode/Toxicity')\n",
        "\n",
        "#GoML\n",
        "sys.path.append('/content/GoML/ClickBait')\n",
        "sys.path.append('/content/GoML/NewsCoverage')\n",
        "sys.path.append('/content/GoML/StanceDetection')\n",
        "sys.path.append('/content/GoML/WritingStyle')\n",
        "sys.path.append('/content/GoML/SupportingFiles')\n",
        "\n",
        "#Sigma\n",
        "sys.path.append('/content/Sigma/Credibility')\n",
        "sys.path.append('/content/Sigma/MaliciousAccount')\n",
        "sys.path.append('/content/Sigma/NetworkBasedPredictor')\n",
        "sys.path.append('/content/Sigma/VerifiableAuthenticity')\n",
        "\n",
        "#Team Seekers\n",
        "sys.path.append('/content/TeamSeekers/BERTMCC')\n",
        "sys.path.append('/content/TeamSeekers/ClickBait')\n",
        "sys.path.append('/content/TeamSeekers/Spam')\n",
        "sys.path.append('/content/TeamSeekers/StanceDetection')\n",
        "\n",
        "#TheFeatureFinders\n",
        "sys.path.append('/content/TheFeatureFinders/ReliableSource')\n",
        "sys.path.append('/content/TheFeatureFinders/StanceDetection')\n",
        "sys.path.append('/content/TheFeatureFinders/Toxicity')\n",
        "sys.path.append('/content/TheFeatureFinders/SupportingFiles')\n",
        "\n",
        "#TheShinningUnicorns\n",
        "sys.path.append('/content/TheShinningUnicorns/SupportingFiles')\n",
        "\n",
        "#Trail Blaizers\n",
        "sys.path.append('/content/Trailblazers/MisleadingIntentions')\n",
        "sys.path.append('/content/Trailblazers/Education')\n",
        "sys.path.append('/content/Trailblazers/EventDetection')\n",
        "\n",
        "print('Success adding paths to the system')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success adding paths to the system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugiVVfbFedXl"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yc9Q96XNZiy",
        "outputId": "fa4607c9-f56f-4494-9e75-223cd1e6e73a"
      },
      "source": [
        "#IMPORTS\n",
        "import torch\n",
        "import nltk\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "from IPython.display import Image, clear_output\n",
        "import en_core_web_md\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True\n",
        "\n",
        "clear_output()\n",
        "print('Success importing dependencies')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success importing dependencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l94D9lHlHC13",
        "outputId": "13775e22-1851-456e-b60b-cd6e88a2b879"
      },
      "source": [
        "# NLTK SPECIFIC DOWNLOADS\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger') # pos_tag\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "clear_output()\n",
        "print('Success downloading NLTK dependencies')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHL9hWcX17qr"
      },
      "source": [
        "#Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "033XN2S9Gd84",
        "outputId": "5881f558-a86f-4323-8059-8ad279dd2294"
      },
      "source": [
        "#INSTANTIATE ALL FACTORS\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "from sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "print('Finish instantiating #Blastoff - Sensationalism')\n",
        "\n",
        "#Blastoff - Content Statistics\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "print('Finish instantiating #Blastoff - Content Statistics')\n",
        "\n",
        "# #Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "print('Finish instantiating #Blastoff - Context Veracity')\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "print('Finish instantiating #GirlsWhoCode - Toxicity')\n",
        "\n",
        "#GirlsWhoCode - Topics\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "print('Finish instantiating #GirlsWhoCode - Topics')\n",
        "\n",
        "#GirlsWhoCode - Bias\n",
        "from bias import Gwc_Bias\n",
        "print('Finish instantiating #GirlsWhoCode - Bias')\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation\n",
        "print('Finish instantiating #GirlsWhoCode - Political Afiliation')\n",
        "\n",
        "# #Feature Finders - Stance Detection\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()\n",
        "print('Finish instantiating #Feature Finders - Stance Detection')\n",
        "\n",
        "# #Feature Finders - Toxicity\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()\n",
        "print('Finish instantiating #Feature Finders - Toxicity')\n",
        "\n",
        "# #Feature Finders - Reliable Source\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()\n",
        "print('Finish instantiating #Feature Finders - Reliable Source')\n",
        "\n",
        "# #Feature Finders - Title vs Body\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "tvb = titlevsbody.TitleVsBody()\n",
        "print('Finish instantiating #Feature Finders - Title vs Body')\n",
        "\n",
        "#Team Seekers - BERT\n",
        "from seekers_bertmcc import Seekers_BertMcc\n",
        "print('Finish instantiating #Team Seekers - BERT')\n",
        "\n",
        "# #Team Seekers - Stance Detection\n",
        "from seekers_stancedetection import Seekers_StanceDetection\n",
        "print('Finish instantiating #Team Seekers - Stance Detection')\n",
        "\n",
        "# #Team Seekers - ClickBait\n",
        "from seekers_clickbait import Seekers_ClickBait\n",
        "print('Finish instantiating #Team Seekers - ClickBait')\n",
        "\n",
        "# #Team Seekers - Spam\n",
        "from seekers_spam import Seekers_Spam\n",
        "print('Finish instantiating #Team Seekers - Spam')\n",
        "\n",
        "# #TrailBlazers - Misleading Intentions\n",
        "import alternusvera_misleading_intentions as MI\n",
        "print('Finish instantiating #TrailBlazers - Misleading Intentions')\n",
        "\n",
        "# #TrailBlazers - Education\n",
        "import alternusvera_education as AVE\n",
        "print('Finish instantiating #TrailBlazers - Education')\n",
        "\n",
        "# #TrailBlazers - Event Coverage\n",
        "import EventCoverage as EC\n",
        "print('Finish instantiating #TrailBlazers - Event Coverage')\n",
        "\n",
        "# #GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "from go_ml import *\n",
        "print('Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style')\n",
        "\n",
        "#Sigma - MaliciousAccount\n",
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/Sigma/MaliciousAccount/malicious_account_MLP.pkl\")\n",
        "print('Finish instantiating #Sigma - MaliciousAccount')\n",
        "\n",
        "# #Sigma - Credibility\n",
        "from SIGMAcredibility  import Credibility\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/Sigma/Credibility/credibility_model.pkl\")\n",
        "print('Finish instantiating #Sigma - Credibility')\n",
        "\n",
        "# #Sigma Network Based Predictor\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/Sigma/NetworkBasedPredictor/networkbased.pkl')\n",
        "print('Finish instantiating #Sigma Network Based Predictor')\n",
        "\n",
        "# #Sigma Verifiable Authenticity\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "va = VerifiableAuthenticity()\n",
        "print('Finish instantiating #Sigma Verifiable Authenticity')\n",
        "\n",
        "\n",
        "\n",
        "# #Ceral Killers - Social Credibility\n",
        "import socialcredibility as SC \n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "print('Finish instantiating #Ceral Killers - Social Credibility')\n",
        "\n",
        "# #Ceral Killers - Psychological Utility\n",
        "# import psychologicalutility as PU\n",
        "# pu = PU.CerealKillers_PsychologicalUtility()\n",
        "# print('Finish instantiating #Ceral Killers - Psychological Utility')\n",
        "\n",
        "# #Ceral Killers - BERT\n",
        "import bert as BT\n",
        "bt = BT.CerealKillers_SentimentClassifier()\n",
        "print('Finish instantiating #Ceral Killers - BERT')\n",
        "\n",
        "# #The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "import finalteamintegration as EM\n",
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()\n",
        "print('Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics')\n",
        "\n",
        "\n",
        "print('\\n\\n#############Success Instantiating All Factors#################')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish instantiating #Blastoff - Sensationalism\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loading default pretrained model.\n",
            "Finish instantiating #Blastoff - Content Statistics\n",
            "Finish instantiating #Blastoff - Context Veracity\n",
            "Finish instantiating #GirlsWhoCode - Toxicity\n",
            "Finish instantiating #GirlsWhoCode - Topics\n",
            "Finish instantiating #GirlsWhoCode - Bias\n",
            "Finish instantiating #GirlsWhoCode - Political Afiliation\n",
            "Finish instantiating #Feature Finders - Stance Detection\n",
            "Finish instantiating #Feature Finders - Toxicity\n",
            "Finish instantiating #Feature Finders - Reliable Source\n",
            "Finish instantiating #Feature Finders - Title vs Body\n",
            "Finish instantiating #Team Seekers - BERT\n",
            "Finish instantiating #Team Seekers - Stance Detection\n",
            "Finish instantiating #Team Seekers - ClickBait\n",
            "Finish instantiating #Team Seekers - Spam\n",
            "Finish instantiating #TrailBlazers - Misleading Intentions\n",
            "Finish instantiating #TrailBlazers - Education\n",
            "Finish instantiating #TrailBlazers - Event Coverage\n",
            "Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
            "Finish instantiating #Sigma - MaliciousAccount\n",
            "Finish instantiating #Sigma - Credibility\n",
            "Finish instantiating #Sigma Network Based Predictor\n",
            "Finish instantiating #Sigma Verifiable Authenticity\n",
            "Finish instantiating #Ceral Killers - Social Credibility\n",
            "Finish instantiating #Ceral Killers - BERT\n",
            "Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics\n",
            "\n",
            "\n",
            "#############Success Instantiating All Factors#################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1FXU_sTeb5"
      },
      "source": [
        "# Polinomial equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5u_4iKQTddb"
      },
      "source": [
        "#Feature finders\n",
        "\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]\n",
        "\n",
        "# Trail Blaizers\n",
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcsZksxrHlFI"
      },
      "source": [
        "def getProbablityScore(category):\n",
        "\n",
        "    if(category =='pants-fire' ):\n",
        "        return 0.9;\n",
        "    elif(category =='false'):\n",
        "        return 0.73;\n",
        "    elif(category =='full-flop' or category == 'barely-true'):\n",
        "        return 0.56;\n",
        "    elif(category =='half-true' or category == 'half-flip'):\n",
        "        return 0.4;\n",
        "    elif(category =='mostly-true'):\n",
        "        return 0.24;\n",
        "    else:\n",
        "        return 0.1;\n",
        "\n",
        "\n",
        "def isFakeNews(headline_text, body, partyaffiliation,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    headline_text = headline_text\n",
        "    #label_encode = label_encode\n",
        "    accur = [0.57, 0.56, 0.58, 0.56,\n",
        "             0.96, 0.64, 0.76, 0.47, \n",
        "             0.64, 0.56, 0.84, \n",
        "             0.87, 0.70, 0.82, 0.85,\n",
        "             0.78, 0.82, 0.48, 0.72,\n",
        "             0.75, 0.77, 0.77, 0.70,\n",
        "             0.26, 0.28, 0.49,\n",
        "             0.82, 0.35, 0.55,\n",
        "             0.58, 0.54] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    \n",
        "    #For Toxicity - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score1 = GirlsWhoCode_Toxicity.getToxicityScore(headline_text)\n",
        "        print(\"GirlsWhoCode - Toxicity Score = \",score1)\n",
        "        prob.append(w[0] * score1)\n",
        "        sumW += w[0]\n",
        "\n",
        "    #For Topic Modelling for LDA - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score2 = Topics_with_LDA_Bigram.getTopicScoreBigramLDAModel(headline_text)\n",
        "        prob.append(w[1] * score2)\n",
        "        print(\"GirlsWhoCode - Topic Score = \",score2)\n",
        "        sumW += w[1] \n",
        "\n",
        "    #For Bias - GWC\n",
        "    if headline_text != \"\":\n",
        "        score3 = Gwc_Bias.get_bias_score(headline_text)\n",
        "        print(\"GirlsWhoCode - Bias Score = \", score3)\n",
        "        prob.append(w[2] * score3)\n",
        "        sumW += w[2]\n",
        "\n",
        "    #For political_affiliaiton - GWC\n",
        "    if headline_text != \"\" and partyaffiliation != \"\":\n",
        "        score4 = Girlswhocode_PoliticalAfiiliation.DATAMINERS_getPartyAffiliationScore(headline_text,partyaffiliation)\n",
        "        print(\"GirlsWhoCode - Political Affilaiiton Score = \", score4)\n",
        "        prob.append(w[3] * score4)\n",
        "        sumW += w[3]\n",
        "\n",
        "    # For Clickbait - Seekers\n",
        "    if ( headline_text!=\"\"):\n",
        "        score5 = Seekers_ClickBait().predict(headline_text)\n",
        "        print(\"TeamSeekers - Clickbait Score = \", score5)\n",
        "        prob.append(w[4] * score5)\n",
        "        sumW += w[4]\n",
        "    \n",
        "    #For Stance_detection - Seekers\n",
        "    if (headline_text != \"\"):\n",
        "        score6 = Seekers_StanceDetection().predict(headline_text)\n",
        "        print(\"TeamSeekers - StanceDetection Score = \", score6)\n",
        "        prob.append(w[5] * score6)\n",
        "        sumW += w[5]\n",
        "\n",
        "    # For Spam - Seekers\n",
        "    if ( headline_text!=\"\"):\n",
        "        score7 = Seekers_Spam().predict(headline_text)\n",
        "        print(\"TeamSeekers - StanceDetection Score = \", score7)\n",
        "        prob.append(w[6] * score7)\n",
        "        sumW += w[6]\n",
        "    \n",
        "    # For Bert - Seekers\n",
        "    if (headline_text!=\"\"):\n",
        "        prediction, score8 = Seekers_BertMcc.get_bert_predictions(headline_text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "        print(\"TeamSeekers - BERT Score = \", score8)\n",
        "        prob.append(w[7] * score8)\n",
        "        sumW += w[7]\n",
        "\n",
        "    # For Education - Trailblazers\n",
        "    if (headline_text != \"\"):\n",
        "        edu_mod_res = AVE.predictLable(headline_text)\n",
        "        score9=getProbablityScore(edu_mod_res)\n",
        "        print(\"Trailblazers - Education score = \",score9)\n",
        "        prob.append(w[8]* score9)\n",
        "        sumW+=w[8]\n",
        "\n",
        "    # For Event Coverage - Trailblazers\n",
        "        event_score = EC.predictLable(headline_text)\n",
        "        print(\"Trailblazers - Event Coverage score = \",event_score[0])\n",
        "        prob.append(w[9]* event_score[0])\n",
        "        sumW+=w[9]\n",
        "\n",
        "    # For Misleding intentions - Trailblazers\n",
        "        mislead_intent = (MI.predictIntention(headline_text))\n",
        "        mislead_score=getProbablityScore(mislead_intent)\n",
        "        print(\"Trailblazers - Misleading Intensions score = \",mislead_score)\n",
        "        prob.append(w[10]* mislead_score)\n",
        "        sumW+=w[10]\n",
        "    \n",
        "    # Stance Detection - Go ML\n",
        "    if body:\n",
        "        score12 = getStanceDetectionScore(body)\n",
        "        print(\"GoML - Misleading Intensions score = \",score12)\n",
        "        prob.append(w[11] * score12)\n",
        "        sumW += w[11]\n",
        "    \n",
        "    # Clickbait - Go ML\n",
        "    if body:\n",
        "        score13 = getClickbaitScore(body)\n",
        "        print(\"GoML - Clickbait score = \",score13)\n",
        "        prob.append(w[12] * score13)\n",
        "        sumW += w[12]\n",
        "    \n",
        "    # News Coverage - Go ML\n",
        "    if body:\n",
        "        score14 = getNewsCoverageScore(body)\n",
        "        print(\"GoML - News Coverage Score = \",score14)\n",
        "        prob.append(w[13] * score14)\n",
        "        sumW += w[13]\n",
        "    \n",
        "    # Writing Style - Go ML\n",
        "    if body:\n",
        "        score15 = WritingStyleScore(body)\n",
        "        print(\"GoML - Writing Style score = \",score15)\n",
        "        prob.append(w[14] * score15)\n",
        "        sumW += w[14]\n",
        "\n",
        "    # Sigma - Malicious Account\n",
        "    if (headline_text != \"\"):\n",
        "        factorMA, _, _  = maObject.predict(malicious_accountCls, body, nlp)\n",
        "        print(\"Sigma - Malicious Account score = \", factorMA)\n",
        "        prob.append(w[15] * factorMA ) \n",
        "        sumW += w[15]\n",
        "        \n",
        "      # credebility - Sigma\n",
        "        _ , factorCR = credObject.predict(credCls, body , nlp)\n",
        "        print(\"Sigma - Credibility score = \", factorCR)\n",
        "        prob.append(w[16] * factorCR ) \n",
        "        sumW += w[16]\n",
        "\n",
        "      # Network base - Sigma\n",
        "        factorNB= nb.predict(body, nlp)\n",
        "        print(\"Sigma - Network Base score = \", factorNB)\n",
        "        prob.append(w[17] * factorNB ) \n",
        "        sumW += w[17]\n",
        "\n",
        "      # Authenticity - Sigma\n",
        "        venue = 'CNN'\n",
        "        factorVA = va.predict(body, venue)\n",
        "        print(\"Sigma - Authenticity score = \", factorVA)\n",
        "        prob.append(w[18] * factorVA ) \n",
        "        sumW += w[18]\n",
        "\n",
        "      # Feature Finders - Stance Detection\n",
        "        score20 = predictStance.FeatureFinders_getStanceScore(headline_text,body)\n",
        "        print(\"Feature Finders - Stance Detection score = \", score20)\n",
        "        prob.append(w[19] * score20)\n",
        "        sumW += w[19]\n",
        "\n",
        "      # Feature Finders - Reliable Source\n",
        "        score21 = reliablesource.FeatureFinders_getReliabilityBySource(source)\n",
        "        print(\"Feature Finders - Reliable Source score = \", score21)\n",
        "        prob.append(w[20] * score21)\n",
        "        sumW += w[20]\n",
        "\n",
        "      # Feature Finders - Toxicity\n",
        "        score22 = predictToxicity.FeatureFinders_getToxicityScore(headline_text,body)\n",
        "        print(\"Feature Finders - Toxicity score = \", score22)\n",
        "        prob.append(w[21] * score22)\n",
        "        sumW += w[21]\n",
        "      \n",
        "      # Feature Finders - Title vs Body\n",
        "        tvb_value = tvb.FeatureFinders_getTitleVsBodyRelationship(head_line=headline_text, body_text=body)[0]\n",
        "        score23 = tvb.FeatureFinders_getTitleVsBodyScore(tvb_value)[0][1]\n",
        "        print(\"Feature Finders - Title vs Body = \", score23)\n",
        "        prob.append(w[22] * score23) # 0 is Fake, 1 is True\n",
        "        sumW += w[22]\n",
        "\n",
        "    # Cereal Killers - Sentiment Classification\n",
        "    if headline_text != \"\":\n",
        "        bt_score = bt.predict(headline_text)\n",
        "        print(\"Cereal Killers -  Sentiment Classification score= \", bt_score)\n",
        "        prob.append(w[26]* bt_score)\n",
        "        sumW+=w[26]\n",
        "\n",
        "    # Cereal Killers -  Psychological Utility\n",
        "    # if body != \"\":\n",
        "    #     pu_score = pu.predict(body,barely_true_count,false_count,half_true_count,mostly_true_count,pants_on_fire_count)\n",
        "    #     print(\"Cereal Killers -  Psychological Utility score = \", pu_score)\n",
        "    #     prob.append(w[27]* pu_score)\n",
        "    #     sumW+=w[27]\n",
        "      \n",
        "    # Cereal Killers - Social Credibility\n",
        "    if source != \"\":\n",
        "        sc.search_user_by_name(source)\n",
        "        sc_score = sc.predict(sc.get_user_data())\n",
        "        print(\"Cereal Killers -  Social Credibility score = \", sc_score)\n",
        "        prob.append(w[28]* sc_score)\n",
        "        sumW+=w[28]\n",
        "\n",
        "    # Blastoff - Content Statistics\n",
        "    if (headline_text != \"\"):\n",
        "        in_df = pd.DataFrame(data=[headline_text], columns=['Statement'])\n",
        "        res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "        print(\"Blastoff - Content Statistics score = \", float(res_cs.iloc[0]))\n",
        "        prob.append(w[23] * float(res_cs.iloc[0]))\n",
        "        sumW+=w[23]\n",
        "\n",
        "    # Blastoff - Context Veracity\n",
        "    if (headline_text != \"\"):\n",
        "        score25 = float(cv.get_veracity_scores(headline_text))\n",
        "        print(\"Blastoff - Context Veracity score = \", score25)\n",
        "        prob.append(w[24]* score25)\n",
        "        sumW+=w[24]\n",
        "\n",
        "    # Blastoff - Sensationalism\n",
        "    if (headline_text != \"\"):\n",
        "        score26 = 0.75 * sensa.getScore(headline_text)\n",
        "        print(\"Blastoff - Sensationalism score = \", score26)\n",
        "        prob.append(w[25] * score26 )\n",
        "        sumW+=w[25]\n",
        "\n",
        "    # The Shinning Unicorns - Content Statistics\n",
        "    if (headline_text != \"\"):\n",
        "         content_Statistics_score = contentStatistics1.predict(headline_text)\n",
        "         print(\"The Shinning Unicorns - Content Statistics score \",content_Statistics_score)\n",
        "         prob.append(w[29]* content_Statistics_score)\n",
        "         sumW+=w[29]\n",
        "\n",
        "    # The Shinning Unicorns - Echo Chamber\n",
        "    if (headline_text != \"\"):\n",
        "         echo_ChamberMaster_score = echoChamberMaster1.predict(headline_text)\n",
        "         print(\"The Shinning Unicorns - Echo Chamber score \",echo_ChamberMaster_score)\n",
        "         prob.append(w[30]* echo_ChamberMaster_score)\n",
        "         sumW+=w[30]\n",
        "\n",
        "    \n",
        "   \n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh7Tk4fG_qg7"
      },
      "source": [
        "#Inference \n",
        "\n",
        "This block shows all the factors' scores combined and independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpY3DW10H1y3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dddf9c3-e046-4850-b2d1-3c0f591fafd2"
      },
      "source": [
        "print(\"\\n\\n########## The score of all factors independently is: #############\")\n",
        "\n",
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes','Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.', 'democrat', 'fox news','in a report', 0,50,50,0,0)\n",
        "\n",
        "print(\"\\n\\n########## The score of all factors combined is: #############\")\n",
        "\n",
        "if(result>=0.8):\n",
        "    print(\"pants-on-fire\")\n",
        "elif(result<0.80 and result>=0.64):\n",
        "    print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "    print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "    print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "    print('mostly-true')\n",
        "else:\n",
        "    print('true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GirlsWhoCode - Toxicity Score =  0.44643759813116957\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "GirlsWhoCode - Topic Score =  0.4305878508964852\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "GirlsWhoCode - Bias Score =  0.9996785585546136\n",
            "GirlsWhoCode - Political Affilaiiton Score =  0.4350559435586251\n",
            "TeamSeekers - Clickbait Score =  0.2769627522965428\n",
            "TeamSeekers - StanceDetection Score =  0.05164497531441823\n",
            "TeamSeekers - StanceDetection Score =  0.6062874084973348\n",
            "TeamSeekers - BERT Score =  0.3306647141774495\n",
            "Trailblazers - Education score =  0.73\n",
            "Trailblazers - Event Coverage score =  0.39483999772424655\n",
            "Trailblazers - Misleading Intensions score =  0.4\n",
            "GoML - Misleading Intensions score =  0.3102341827891003\n",
            "GoML - Clickbait score =  0.3102341827891003\n",
            "GoML - News Coverage Score =  0.3102341827891003\n",
            "GoML - Writing Style score =  0.0\n",
            "Sigma - Malicious Account score =  1\n",
            "Sigma - Credibility score =  0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:00, 165.67it/s]\n",
            "1it [00:00, 1218.57it/s]\n",
            "1it [00:00, 315.57it/s]\n",
            "1it [00:00, 148.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sigma - Network Base score =  0.16\n",
            "Sigma - Authenticity score =  0.4391602289646022\n",
            "Feature Finders - Stance Detection score =  0.7445432720672169\n",
            "Feature Finders - Reliable Source score =  0.5002441826895172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Finders - Toxicity score =  0.4741172194480896\n",
            "\n",
            "predicted titleVsBody: [2]\n",
            "Feature Finders - Title vs Body =  0.6302343574626981\n",
            "Cereal Killers -  Sentiment Classification score=  1\n",
            "Cereal Killers -  Social Credibility score =  1\n",
            "Blastoff - Content Statistics score =  0.8\n",
            "Blastoff - Context Veracity score =  3.0\n",
            "Blastoff - Sensationalism score =  3.0\n",
            "The Shinning Unicorns - Content Statistics score  0.5290934778864554\n",
            "The Shinning Unicorns - Echo Chamber score  0.832\n",
            "barely-true\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}