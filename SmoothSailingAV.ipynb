{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmoothSailingAV.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTZPGteW0AVvMfQRP4R8YS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/257_Final_Exam/blob/main/SmoothSailingAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxn0Z5m127G"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI9u-xyA-7L"
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xETwjdcu3rDo",
        "outputId": "6feb1206-29fd-45b4-e173-478bd97d972b"
      },
      "source": [
        "#UNSINTALL\n",
        "#Feature finders\n",
        "!pip uninstall -y xgboost\n",
        "clear_output()\n",
        "print('Success Uninstalling libraries')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVyOfyY3ov_",
        "outputId": "1da2729c-30b2-4667-eed4-8e805c69e6a2"
      },
      "source": [
        "#WGET\n",
        "\n",
        "#Team Seakers\n",
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
        "!unzip bertmodel\n",
        "\n",
        "#FEATURE FINDERS\n",
        "!wget https://mirrors.aliyun.com/pypi/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl#sha256=898f26bb66589c644d17deff1b03961504f7ad79296ed434d0d7a5e9cb4deae6\n",
        "\n",
        "clear_output()\n",
        "print('Success Downloading files')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Downloading files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXWz4Ke5xF1x",
        "outputId": "00dc65e1-1ac5-4f02-b1a5-8a4f41fd662f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/abenassi/Google-Search-API (from -r requirements.txt (line 16))\n",
            "  Cloning https://github.com/abenassi/Google-Search-API to /tmp/pip-req-build-ce1up7w9\n",
            "  Running command git clone -q https://github.com/abenassi/Google-Search-API /tmp/pip-req-build-ce1up7w9\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.2.4)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.6.3)\n",
            "Collecting selenium<3.0.0,>=2.44.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/f8/e46684764161fee7b7d2e0fd9b82d56915438b68e498181dc45183643c82/selenium-2.53.6-py2.py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 22.1MB/s \n",
            "\u001b[?25hCollecting vcrpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/62/571e9fa5c2a2c986c001d1be99403a5e800d2e72b905e6b1e951148c75c9/vcrpy-4.1.1-py2.py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.16.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 16.0MB/s \n",
            "\u001b[?25hCollecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/23/6f5addc2ade7c6240e2c9169bd7a9506cea17b35c9f322104a60dd4ba7fd/ktrain-0.25.2.tar.gz (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 1.3MB/s \n",
            "\u001b[?25hCollecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 48.8MB/s \n",
            "\u001b[?25hCollecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/b1/ab40a00b727a0d209402d1be6aa3f1bc75bd03678b59ace8507b08bf12f5/textstat-0.7.0-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (3.6.0)\n",
            "Processing ./xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.7.0+cu101)\n",
            "Requirement already satisfied: transformers==3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (3.3.0)\n",
            "Collecting autogluon==0.0.15b20201205\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/4e/2094d275f0a973bcf7c0e523e920fc7731190a610bef4801ffe85288b984/autogluon-0.0.15b20201205-py3-none-any.whl\n",
            "Requirement already satisfied: mxnet-cu100 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 34)) (1.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (50.3.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 9)) (2020.11.8)\n",
            "Collecting yarl; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 11)) (3.13)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 11)) (1.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 14)) (2.8.1)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 14)) (7.0.0)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 14)) (4.2.6)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 13.6MB/s \n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (1.1.4)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (1.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (0.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (20.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (5.5.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 65.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (0.42.1)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e5/a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382/cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 63.0MB/s \n",
            "\u001b[?25hCollecting syntok\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/76/a49e73a04b3e3a14ce232e8e28a1587f8108baa665644fe8c40e307e792e/syntok-1.3.1.tar.gz\n",
            "Collecting seqeval==0.0.19\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e5/b7705156a77f742cfe4fc6f22d0c71591edb2d243328dff2f8fc0f933ab6/seqeval-0.0.19.tar.gz\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (0.1.94)\n",
            "Collecting keras_bert>=0.86.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain->-r requirements.txt (line 20)) (2.5)\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 56.3MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 25)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r requirements.txt (line 25)) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 30)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 30)) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3->-r requirements.txt (line 31)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3->-r requirements.txt (line 31)) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3->-r requirements.txt (line 31)) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3->-r requirements.txt (line 31)) (0.8.1rc2)\n",
            "Collecting autogluon.core==0.0.15b20201205\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/a0/8fb14f0572142625e756386ee030c251a4b4123090835d2a44d957e64723/autogluon.core-0.0.15b20201205-py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 60.1MB/s \n",
            "\u001b[?25hCollecting autogluon.vision==0.0.15b20201205\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/f2/8899da73c67899a7373aa5305fd076878ea5340f03b23cd90ce7ab056ecd/autogluon.vision-0.0.15b20201205-py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (3.6.4)\n",
            "Collecting autogluon.text==0.0.15b20201205\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/62/2dfa92ee7da2e05b6cfdf47285f891a2d12e30346577516bded2de42368b/autogluon.text-0.0.15b20201205-py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hCollecting autogluon.extra==0.0.15b20201205\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/a7/fe155e50317fada8a2728dee9c6648e1d3935ca724c4b3739446e4dbacdb/autogluon.extra-0.0.15b20201205-py3-none-any.whl\n",
            "Collecting autogluon.tabular==0.0.15b20201205\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ea/c0c3d680657e5f15acd5cfd4a8ace44deddd819362d984e8b012c7dd8e8f/autogluon.tabular-0.0.15b20201205-py3-none-any.whl (300kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 61.3MB/s \n",
            "\u001b[?25hCollecting autogluon.mxnet==0.0.15b20201205\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/0e/5bdbdac07d0c51f05188db416bd5018085b54064103e116d17484c6019ce/autogluon.mxnet-0.0.15b20201205-py3-none-any.whl\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100->-r requirements.txt (line 34)) (0.8.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 2)) (2.0.0)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 69.1MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain->-r requirements.txt (line 20)) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain->-r requirements.txt (line 20)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain->-r requirements.txt (line 20)) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain->-r requirements.txt (line 20)) (0.7.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.19->ktrain->-r requirements.txt (line 20)) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3->-r requirements.txt (line 31)) (7.1.2)\n",
            "Collecting paramiko>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 58.9MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/65/a573dd1cc6ca76815f358f9e5be449221f4017a87b2e6b5ea970df28f94b/boto3-1.16.33-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 61.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.12.0)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.3)\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.29.21)\n",
            "Collecting distributed>=2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/38/d9f0e31c15de18cb124d1ed33cf9c99c84f05f251ff6767e7573c217725b/distributed-2.30.1-py3-none-any.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (5.1.1)\n",
            "Collecting ConfigSpace<=0.4.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/f9/685e3cb6e2a87e4989b83950dfbe8cecc49fd158f4ff3d1368dc62014e8d/ConfigSpace-0.4.16.tar.gz (964kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 52.6MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5d/30ce4d1af609ba389d55654e6a7271619253dbbe7006a33bb20c703f0234/openml-0.11.0.tar.gz (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 60.6MB/s \n",
            "\u001b[?25hCollecting d8<1.0,>=0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/13/c6c136086e09350446c65399327eac3d2fdf1f0a1e6217804bdd0e70ecb4/d8-0.0.2-py3-none-any.whl\n",
            "Collecting gluoncv==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/d7/e6429bd8d6ee2ff8123f7afc197a5a60901bbf9afc47e1f30c484cf22540/gluoncv-0.9.0-py2.py3-none-any.whl (997kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 64.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (8.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.9.0)\n",
            "Collecting autogluon-contrib-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/dc/30d76d44709cf87e9ae9bc9ed8c9ce1882436427ded27edced390ed2933f/autogluon_contrib_nlp-0.0.1b20201009-py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 62.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<=1.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.text==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.14.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from autogluon.extra==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.1.1)\n",
            "Collecting lightgbm<4.0,>=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/cd/2b7783e8c250f8191b72e9a0010e0429a799d3305c27764d7bf113dfd078/lightgbm-3.1.1-py2.py3-none-manylinux1_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 72.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil<=5.7.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from autogluon.tabular==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (5.4.8)\n",
            "Collecting catboost<0.25,>=0.23.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/c1/c1c4707013f9e2f8a96899dd3a87f66c9167d6d776a6dc8fe7ec8678d446/catboost-0.24.3-cp36-none-manylinux1_x86_64.whl (66.3MB)\n",
            "\u001b[K     |████████████████████████████████| 66.3MB 80kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain->-r requirements.txt (line 20)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain->-r requirements.txt (line 20)) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain->-r requirements.txt (line 20)) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain->-r requirements.txt (line 20)) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Collecting pynacl>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
            "\u001b[K     |████████████████████████████████| 962kB 46.4MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/de/7054df0620b5411ba45480f0261e1fb66a53f3db31b28e3aa52c026e72d9/cryptography-3.3.1-cp36-abi3-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 53.5MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.33\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/3e/5f64c7b492312069520d219ed56ce6b3d1821dab8251c18d25728578b58e/botocore-1.19.33-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 50.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.0.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.3.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.11.1)\n",
            "Collecting contextvars; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.0.0)\n",
            "Collecting liac-arff>=2.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.5.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gluoncv==0.9.0->autogluon.vision==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (4.1.2.30)\n",
            "Collecting yacs\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting decord\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/0c/7d99cfcde7b85f80c9ea9b0b19441339ad3cef59ee7fa5386598db714efe/decord-0.4.2-py2.py3-none-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 44.5MB/s \n",
            "\u001b[?25hCollecting autocfg\n",
            "  Downloading https://files.pythonhosted.org/packages/94/4d/71221541472d0e45351cf9b11b0c3d55c032b80dcf8bc0763b28791e3dd3/autocfg-0.0.6-py2.py3-none-any.whl\n",
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 61.5MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ca/3971802ee6251da1abead1a22831d7f4743781e2f743bd266bdd2f46c19b/flake8-3.8.4-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from autogluon-contrib-nlp->autogluon.text==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (3.12.4)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->autogluon.extra==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.11.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.35.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost<0.25,>=0.23.0->autogluon.tabular==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (4.4.1)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.14.3)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.0.1)\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (0.0.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (4.0.1)\n",
            "Collecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25hCollecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->autogluon.extra==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.1.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost<0.25,>=0.23.0->autogluon.tabular==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.3.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (2.20)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.0.15b20201205->autogluon==0.0.15b20201205->-r requirements.txt (line 33)) (1.3)\n",
            "Building wheels for collected packages: ConfigSpace\n",
            "  Building wheel for ConfigSpace (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.16-cp36-cp36m-linux_x86_64.whl size=2881451 sha256=c3d511efaaec9cd1a2179062cc8c2b729bf59c4906d01823e7697298348f9e1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/13/2f/a0ae8e1081410d394133980a2ccd1c5524ed4f0c83ba9a61a7\n",
            "Successfully built ConfigSpace\n",
            "Building wheels for collected packages: fake-useragent, ktrain, Google-Search-API, feedfinder2, tinysegmenter, jieba3k, langdetect, syntok, seqeval, keras-bert, sgmllib3k, keras-transformer, openml, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, contextvars, liac-arff, keras-self-attention\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=2572ebe5ba8acccd13dcb6003d5c00201b7b4af2e739c546de951d39c9996e2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.25.2-cp36-none-any.whl size=25276306 sha256=1091bbe3ff82c41a1bce8511d1ba4881b84d86faa23f447c321dc9b80f5a2a4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/56/00/25444c352cc843e5c5daea0e9517a192878ae22c2c6b5f4573\n",
            "  Building wheel for Google-Search-API (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Google-Search-API: filename=Google_Search_API-1.1.14-cp36-none-any.whl size=607835 sha256=7c7463a47a3addd4c50cec83feb9d091eb5787ecfeac28172b75254bccc171f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7mitvdy7/wheels/9d/dd/52/f744758aa94909328835af5d6f1650c9c3d8cf2e6e8988767a\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=f2e7e176c33b81e1009902d9f68a0c604c009a22ebb0e0c2cdbcfe0e3cb145c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=fcb69de1dc86e5ec1436bea5e164e3e74d18079c75dc9db5723bf3aa0ec628f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=20cb246228947600aa16909c3759b280dc2f02f2e68e13090fa3a48c27c705c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=50847c514ea47ea3bb086bc6a96d1210a0bd704c01d428df6905fd908cfbfb22\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.3.1-cp36-none-any.whl size=20919 sha256=61ab9548eba41c89177da2e31c3ef4d3fc6a962d03fb971c5f6c2c54fd226a3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/c6/a4/be1920586c49469846bcd2888200bdecfe109ec421dab9be2d\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.19-cp36-none-any.whl size=9919 sha256=8930f7a4c33be040d678c34f2c8f5b9ccbc8cc44a881315763f19b7a9f22a103\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/bf/1198beceed805a2099060975f6281d1b01046dd279e19c97be\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=a2b95d0a24bcd167a30f2f9bf21f4955ba72b1ebc854bfceaa3aebe1f6298586\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=0d8fe591a223a89778eccde9265489ab94a85d6c0a008ea4be4fe3b113154909\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=99e95b2ed07f2904f316447e7efd489d2e5e4ade9fbddb5de3b3a09301d9c3e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.11.0-cp36-none-any.whl size=127467 sha256=625babfba1476a7f5eb81df46b2f9342cc962243125f8be941b2193299a68415\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/29/f1/58115101bafad19069e838060ab6bdd8046abceba508500e03\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=ca2fab414c4ea3e5c33cdda75c953c4a0c8806b02281c4e26cd347def7962c58\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=0448288f01678b2e6766de2e0014294160aaed9e69602faaf794b5006f154c03\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=80071f60a7a7c77359ecee131dda740e1331ccecbc9705628840b25fddb45426\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=cb4fc3d5b5310211ac41a10075be70e3e0b78deebc811f07efb7134907305447\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=4abadecbe74906504c371e7ae9494ed1070e09d8ce4521c5bda0b690d5022b86\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=7d3818119bcf285c84c0c776acbbdf22bb686eea90b8c47c69b34199f5def247\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp36-none-any.whl size=11734 sha256=cda25a4c45a0cd7a05021cede420c3f4292e74e8a471811ed16feb1079ff166a\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=f5871ae2f72bfc2865da54759f69238c43c8dccc564e72c43410a87ea65a9c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built fake-useragent ktrain Google-Search-API feedfinder2 tinysegmenter jieba3k langdetect syntok seqeval keras-bert sgmllib3k keras-transformer openml keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim contextvars liac-arff keras-self-attention\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.19.33 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: autogluon-mxnet 0.0.15b20201205 has requirement Pillow<=6.2.1, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: autogluon-vision 0.0.15b20201205 has requirement Pillow<=6.2.1, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: autogluon-tabular 0.0.15b20201205 has requirement xgboost<1.3,>=1.2, but you'll have xgboost 0.90 which is incompatible.\u001b[0m\n",
            "Installing collected packages: selenium, unidecode, multidict, yarl, vcrpy, fake-useragent, feedfinder2, cssselect, tinysegmenter, sgmllib3k, feedparser, jieba3k, requests-file, tldextract, newspaper3k, langdetect, cchardet, syntok, seqeval, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, whoosh, ktrain, vaderSentiment, pyphen, textstat, xgboost, pynacl, cryptography, bcrypt, paramiko, jmespath, botocore, s3transfer, boto3, pyaml, scikit-optimize, immutables, contextvars, cloudpickle, distributed, ConfigSpace, autogluon.core, liac-arff, xmltodict, openml, autogluon.mxnet, d8, yacs, decord, autocfg, tensorboardx, portalocker, gluoncv, autogluon.vision, mccabe, pycodestyle, pyflakes, flake8, sacrebleu, autogluon-contrib-nlp, autogluon.text, autogluon.extra, lightgbm, catboost, autogluon.tabular, autogluon, Google-Search-API\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed ConfigSpace-0.4.16 Google-Search-API-1.1.14 autocfg-0.0.6 autogluon-0.0.15b20201205 autogluon-contrib-nlp-0.0.1b20201009 autogluon.core-0.0.15b20201205 autogluon.extra-0.0.15b20201205 autogluon.mxnet-0.0.15b20201205 autogluon.tabular-0.0.15b20201205 autogluon.text-0.0.15b20201205 autogluon.vision-0.0.15b20201205 bcrypt-3.2.0 boto3-1.16.33 botocore-1.19.33 catboost-0.24.3 cchardet-2.1.7 cloudpickle-1.6.0 contextvars-2.4 cryptography-3.3.1 cssselect-1.1.0 d8-0.0.2 decord-0.4.2 distributed-2.30.1 fake-useragent-0.1.11 feedfinder2-0.0.4 feedparser-6.0.2 flake8-3.8.4 gluoncv-0.9.0 immutables-0.14 jieba3k-0.35.1 jmespath-0.10.0 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 ktrain-0.25.2 langdetect-1.0.8 liac-arff-2.5.0 lightgbm-3.1.1 mccabe-0.6.1 multidict-5.1.0 newspaper3k-0.2.8 openml-0.11.0 paramiko-2.7.2 portalocker-2.0.0 pyaml-20.4.0 pycodestyle-2.6.0 pyflakes-2.2.0 pynacl-1.4.0 pyphen-0.10.0 requests-file-1.5.1 s3transfer-0.3.3 sacrebleu-1.4.14 scikit-optimize-0.8.1 selenium-2.53.6 seqeval-0.0.19 sgmllib3k-1.0.0 syntok-1.3.1 tensorboardx-2.1 textstat-0.7.0 tinysegmenter-0.3 tldextract-3.1.0 unidecode-1.1.1 vaderSentiment-3.3.2 vcrpy-4.1.1 whoosh-2.7.4 xgboost-0.90 xmltodict-0.12.0 yacs-0.1.8 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5itgK72D2GDh",
        "outputId": "569211b7-ee34-44b3-cc16-09f893ea7a50"
      },
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Blastoff\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "#Sigma\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt-get install protobuf-compiler\n",
        "\n",
        "#Cereal Killers\n",
        "!apt-get install git-lfs\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "# clear_output()\n",
        "# print('Success Installing Libraries')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Installing Libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a7blI8t4f7c"
      },
      "source": [
        "#RESTART RUNTIME\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn3j9U816Bv"
      },
      "source": [
        "# Download Repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEZFScB0osBe"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAd_rjrAbTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620a7f3e-5e1d-4b94-fb85-aeeed57302c9"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "!git clone https://github.com/Wayne122/Content_Statistics_Demo.git\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "!git clone https://github.com/roanacla/nlp_sensationalism.git\n",
        "\n",
        "#Blastoff - Context Veracity\n",
        "!git clone https://github.com/snarvekark/Veracity_Factor.git\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='1aNCQPXSpxnTUbqkO1Jvb7c2naTpaCkJ5',\n",
        "                                  dest_path='./toxicityclass1.py',\n",
        "                                  unzip=False)\n",
        "#GirlsWhoCode - Topics\n",
        "gdd.download_file_from_google_drive(file_id='1Fe9s1tzkxyI1AWqWTwhkhaZaGD34kLCz',\n",
        "                                  dest_path='./Topics_with_LDA_Bigram.py',\n",
        "                                  unzip=False)\n",
        "#GirlsWhoCode - Bias\n",
        "gdd.download_file_from_google_drive(file_id='18BVbpsbJ_JIWWzdKem1KJMcq89npmBNF',\n",
        "                                  dest_path='./bias.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "gdd.download_file_from_google_drive(file_id='10BFSTGTiUtTTkq_FiA_ktbs0b05sUTp1',\n",
        "                                  dest_path='./girlswhocode_political_affiliation.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Feature Finders - Stance Detection\n",
        "gdd.download_file_from_google_drive(file_id='1vmn6f-KE31jIZ2gANAm9bUV6BZvJ3HCC',\n",
        "                                  dest_path='./AlternusVeraStanceDetection.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "gdd.download_file_from_google_drive(file_id='1YmNAPeGwX3lGI_qrMIqWKTrhuEZ6wfGD',\n",
        "                                  dest_path='./alternusveratoxicity.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Feature Finders - Reliable Source\n",
        "gdd.download_file_from_google_drive(file_id='14p0H4KIDNl8ICZKsC25vHvrarnlwzoiY',\n",
        "                                  dest_path='./AlternusVeraReliableSource.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Feature Finders - Title vs Body\n",
        "gdd.download_file_from_google_drive(file_id='1CyMWLWWksLLKlDkqtdSQNBJNUnwirwYS',\n",
        "                                  dest_path='./team_features_finders_factor_title_vs_body.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Team Seakers - BERTMCC\n",
        "gdd.download_file_from_google_drive(file_id='104z4L5oaSJuNOo8--j3HYVBIqKLy8kzT',\n",
        "                                  dest_path='./seekers_bertmcc.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Team Seakers - Stance Detection\n",
        "gdd.download_file_from_google_drive(file_id='18xB6HkP_hc8oHz2B8Gv2RctxST-dvl0l',\n",
        "                                  dest_path='./seekers_stancedetection.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Team Seakers - Click Bait\n",
        "gdd.download_file_from_google_drive(file_id='1bnMx6EnB5OCFemHnoyQJg6rDWFI02BPn',\n",
        "                                  dest_path='./seekers_clickbait.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Team Seakers - Spam\n",
        "gdd.download_file_from_google_drive(file_id='1foTcE9fWF6aaEndSe_nNrWIPOh15jzCz',\n",
        "                                  dest_path='./seekers_spam.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "\n",
        "#TrailBlazers - Misleading Intentions\n",
        "!git clone https://github.com/jignesh-sjsu/AlternusVera_MisleadingIntention.git\n",
        "\n",
        "#TrailBlazers - Education\n",
        "!git clone https://github.com/pankajhpatil/AlternusVera_Education.git\n",
        "\n",
        "#TrailBlazers - Event Coverage\n",
        "!git clone https://github.com/Manish0112/AlternusVera_EventDetection.git\n",
        "\n",
        "#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "gdd.download_file_from_google_drive(file_id='1n8YWeStJV4OTfccwEli57PS9pB5SzNmv',\n",
        "                                  dest_path='./go_ml.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "#Sigma - MaliciousAccount, Credibility, Network Based Predictor, Verifiable Authenticity\n",
        "!git clone https://github.com/sacefe/cmpe257_AlternusVera_SIGMA.git\n",
        "\n",
        "\n",
        "#Cereal Killers - Social Credibility, Psychological Utility, BERT \n",
        "!git clone https://github.com/jedvillan/CerealKillers_AlternusVera.git\n",
        "\n",
        "#The Shining Unicorns\n",
        "gdd.download_file_from_google_drive(file_id='1T04flloQktz6hOEOoboHvG2YzA_CcKTT',\n",
        "                                  dest_path='./finalteamintegration.py',\n",
        "                                  unzip=False)\n",
        "\n",
        "clear_output()\n",
        "print('Success clonning repos and downloding files')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success clonning repos and downloding files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8WcHhZe6sm",
        "outputId": "b00c7bac-9d62-4d5d-9740-ef1e37dd0e60"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "#Blastoff\n",
        "sys.path.append('/content/Content_Statistics_Demo')\n",
        "sys.path.append('/content/nlp_sensationalism')\n",
        "sys.path.append('/content/Veracity_Factor')\n",
        "\n",
        "#Trail Blaizers\n",
        "sys.path.append('/content/AlternusVera_MisleadingIntention')\n",
        "sys.path.append('/content/AlternusVera_Education')\n",
        "sys.path.append('/content/AlternusVera_EventDetection')\n",
        "\n",
        "#Cereal Killers\n",
        "sys.path.append('/content/cmpe257_AlternusVera_SIGMA')\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/BERT')\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/SC')\n",
        "sys.path.append('/content/CerealKillers_AlternusVera/PU')\n",
        "\n",
        "print('Success adding paths to the system')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success adding paths to the system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yc9Q96XNZiy",
        "outputId": "18f87189-3c33-47ba-dac1-079ec4da9448"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "import en_core_web_md\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True\n",
        "\n",
        "clear_output()\n",
        "print('Success importing dependencies')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success importing dependencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHL9hWcX17qr"
      },
      "source": [
        "#Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "033XN2S9Gd84",
        "outputId": "cd9adea5-a817-45cf-a7c3-349daddd8d01"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "from sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "\n",
        "#Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "\n",
        "#GirlsWhoCode - Topics\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "\n",
        "#GirlsWhoCode - Bias\n",
        "from bias import Gwc_Bias\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation\n",
        "\n",
        "#Feature Finders - Stance Detection\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()\n",
        "\n",
        "#Feature Finders - Reliable Source\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()\n",
        "\n",
        "#Feature Finders - Title vs Body\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "tvb = titlevsbody.TitleVsBody()\n",
        "\n",
        "#Team Seekers - BERT\n",
        "from seekers_bertmcc import Seekers_BertMcc\n",
        "\n",
        "#Team Seekers - Stance Detection\n",
        "from seekers_stancedetection import Seekers_StanceDetection\n",
        "\n",
        "#Team Seekers - ClickBait\n",
        "from seekers_clickbait import Seekers_ClickBait\n",
        "\n",
        "#Team Seekers - Spam\n",
        "from seekers_spam import Seekers_Spam\n",
        "\n",
        "#TrailBlazers - Misleading Intentions\n",
        "import alternusvera_misleading_intentions as MI\n",
        "\n",
        "#TrailBlazers - Education\n",
        "import alternusvera_education as AVE\n",
        "\n",
        "#TrailBlazers - Event Coverage\n",
        "import EventCoverage as EC\n",
        "\n",
        "#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "from go_ml import *\n",
        "\n",
        "#Sigma - MaliciousAccount\n",
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/malicious_account_MLP.pkl\")\n",
        "\n",
        "#Sigma - Credibility\n",
        "from SIGMAcredibility  import Credibility\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/credibility_model.pkl\")\n",
        "\n",
        "#Sigma Network Based Predictor\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/cmpe257_AlternusVera_SIGMA/networkbased.pkl')\n",
        "\n",
        "#Sigma Verifiable Authenticity\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "va = VerifiableAuthenticity()\n",
        "\n",
        "#Ceral Killers - Social Credibility, Psychological Utility, BERT\n",
        "import socialcredibility as SC \n",
        "import psychologicalutility as PU\n",
        "import bert as BT\n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "pu = PU.CerealKillers_PsychologicalUtility()\n",
        "bt = BT.CerealKillers_SentimentClassifier()\n",
        "\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "import finalteamintegration as EM\n",
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()\n",
        "\n",
        "\n",
        "clear_output()\n",
        "print('Success Instantiating Factors')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loading default pretrained model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-61a9ea49268f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#Feature Finders - Title vs Body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mLABELS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'agree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disagree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'discuss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unrelated'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mteam_features_finders_factor_title_vs_body\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtitlevsbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mtvb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitlevsbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTitleVsBody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/team_features_finders_factor_title_vs_body.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWord2VecFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/MLFall2020/the-feature-finders/datasets/fakenewschallenge/GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/team_features_finders_factor_title_vs_body.py\u001b[0m in \u001b[0;36mWord2VecFeatureGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWord2VecFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatureGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/MLFall2020/the-feature-finders/datasets/fakenewschallenge/GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word2vecFeatureGenerator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/MLFall2020/the-feature-finders/datasets/fakenewschallenge/GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1FXU_sTeb5"
      },
      "source": [
        "# Before Poli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5u_4iKQTddb"
      },
      "source": [
        "#Feature finders\n",
        "\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]\n",
        "\n",
        "# Trail Blaizers\n",
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfTVRNjS195_"
      },
      "source": [
        "#Remove content"
      ]
    }
  ]
}