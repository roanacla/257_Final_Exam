{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmoothSailingAV.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPaj4M12dwXKTdo3wIn3bxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/257_Final_Exam/blob/main/SmoothSailingAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxn0Z5m127G"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI9u-xyA-7L"
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETwjdcu3rDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ed244a-2085-4faa-c09c-62e5b7ea10ae"
      },
      "source": [
        "#UNSINTALL\n",
        "#Feature finders\n",
        "!pip uninstall -y xgboost\n",
        "clear_output()\n",
        "print('Success Uninstalling libraries')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_FTsPY_bfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cec198-4a46-4453-ad50-bb3661c6d8f8"
      },
      "source": [
        "%rm -r *\n",
        "!git clone https://@github.com/roanacla/257_Final_Exam.git\n",
        "%mv 257_Final_Exam/* /content/\n",
        "%rm -r 257_Final_Exam"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '257_Final_Exam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 338 (delta 33), reused 84 (delta 22), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (338/338), 95.45 MiB | 31.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Checking out files: 100% (118/118), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVyOfyY3ov_",
        "outputId": "5981ab0e-df99-4a28-a7e6-7e125afaf25a"
      },
      "source": [
        "#WGET\n",
        "\n",
        "#Team Seakers\n",
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
        "%mkdir /content/TeamSeekers/SupportingFiles\n",
        "%mv bertmodel /content/TeamSeekers/SupportingFiles/\n",
        "!unzip /content/TeamSeekers/SupportingFiles/bertmodel -d /content/TeamSeekers/SupportingFiles/\n",
        "%rm /content/TeamSeekers/SupportingFiles/bertmodel\n",
        "\n",
        "#FEATURE FINDERS\n",
        "!wget --directory-prefix=/content/TheFeatureFinders/SupportingFiles/ https://mirrors.aliyun.com/pypi/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl#sha256=898f26bb66589c644d17deff1b03961504f7ad79296ed434d0d7a5e9cb4deae6\n",
        "\n",
        "clear_output()\n",
        "print('Success Downloading files')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Downloading files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5itgK72D2GDh",
        "outputId": "6874a03b-ef44-4a6e-8c35-e295166348c0"
      },
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Blastoff\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "#Sigma\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt-get install protobuf-compiler\n",
        "\n",
        "#Cereal Killers\n",
        "!apt-get install git-lfs\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "clear_output()\n",
        "print('Success Installing Libraries')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Installing Libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a7blI8t4f7c"
      },
      "source": [
        "#RESTART RUNTIME - THIS IS A REQUIREMENT BEFORE CONTINUING\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn3j9U816Bv"
      },
      "source": [
        "# Download Large Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEZFScB0osBe"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from IPython.display import Image, clear_output\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAd_rjrAbTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41c6ab7-7e07-4790-b1a1-764b7d2f3d40"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "# !git clone https://github.com/Wayne122/Content_Statistics_Demo.git\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "# !git clone https://github.com/roanacla/nlp_sensationalism.git\n",
        "\n",
        "#Blastoff - Context Veracity\n",
        "# !git clone https://github.com/snarvekark/Veracity_Factor.git\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "# gdd.download_file_from_google_drive(file_id='1aNCQPXSpxnTUbqkO1Jvb7c2naTpaCkJ5',\n",
        "#                                   dest_path='./toxicityclass1.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='11g4TSsDJ0ojvkua-BBPd7OVPk3o0QEL1',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.preproc',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1GsgExCNM9pCnAB_gAKPxrwDmg03LZhT-',\n",
        "                                  dest_path='/content/GirlsWhoCode/SupportingFiles/BERTOnLiarLiar/tf_model.h5',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "#GirlsWhoCode - Topics\n",
        "# gdd.download_file_from_google_drive(file_id='1Fe9s1tzkxyI1AWqWTwhkhaZaGD34kLCz',\n",
        "#                                   dest_path='./Topics_with_LDA_Bigram.py',\n",
        "#                                   unzip=False)\n",
        "#GirlsWhoCode - Bias\n",
        "# gdd.download_file_from_google_drive(file_id='18BVbpsbJ_JIWWzdKem1KJMcq89npmBNF',\n",
        "#                                   dest_path='./bias.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "# gdd.download_file_from_google_drive(file_id='1gyQ2zdQ0OS0MWYl1-SCEX1E1CezgKuGH',\n",
        "#                                   dest_path='./GirlsWhoCode.zip',\n",
        "#                                   unzip=False)\n",
        "# with ZipFile('GirlsWhoCode.zip', 'r') as zipObj:\n",
        "#    zipObj.extractall('./GirlsWhoCode/')\n",
        "# gdd.download_file_from_google_drive(file_id='10BFSTGTiUtTTkq_FiA_ktbs0b05sUTp1',\n",
        "#                                   dest_path='./girlswhocode_political_affiliation.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Stance Detection\n",
        "# gdd.download_file_from_google_drive(file_id='1vmn6f-KE31jIZ2gANAm9bUV6BZvJ3HCC',\n",
        "#                                   dest_path='./AlternusVeraStanceDetection.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "# gdd.download_file_from_google_drive(file_id='1YmNAPeGwX3lGI_qrMIqWKTrhuEZ6wfGD',\n",
        "#                                   dest_path='./alternusveratoxicity.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1RszezV7kgo1Abbh5tgW9MlwW6oq1D8Ue',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/model_dbow',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#Feature Finders - Reliable Source\n",
        "# gdd.download_file_from_google_drive(file_id='14p0H4KIDNl8ICZKsC25vHvrarnlwzoiY',\n",
        "#                                   dest_path='./AlternusVeraReliableSource.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Title vs Body\n",
        "# gdd.download_file_from_google_drive(file_id='1CyMWLWWksLLKlDkqtdSQNBJNUnwirwYS',\n",
        "#                                   dest_path='./team_features_finders_factor_title_vs_body.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature finders\n",
        "gdd.download_file_from_google_drive(file_id='19lFvSh2DIs9QAKQ9JbjySH4psWutny_a',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/GoogleNews-vectors-negative300.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "# gdd.download_file_from_google_drive(file_id='1isyawf2DG4TiQw2eLtt4_c6FfjYwZL_7',\n",
        "#                                   dest_path='./titlevsbody_logistic_proba.pkl',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - BERTMCC\n",
        "# gdd.download_file_from_google_drive(file_id='104z4L5oaSJuNOo8--j3HYVBIqKLy8kzT',\n",
        "#                                   dest_path='./seekers_bertmcc.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Stance Detection\n",
        "# gdd.download_file_from_google_drive(file_id='18xB6HkP_hc8oHz2B8Gv2RctxST-dvl0l',\n",
        "#                                   dest_path='./seekers_stancedetection.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Click Bait\n",
        "# gdd.download_file_from_google_drive(file_id='1bnMx6EnB5OCFemHnoyQJg6rDWFI02BPn',\n",
        "#                                   dest_path='./seekers_clickbait.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Spam\n",
        "# gdd.download_file_from_google_drive(file_id='1foTcE9fWF6aaEndSe_nNrWIPOh15jzCz',\n",
        "#                                   dest_path='./seekers_spam.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "\n",
        "#TrailBlazers - Misleading Intentions\n",
        "# !git clone https://github.com/jignesh-sjsu/AlternusVera_MisleadingIntention.git\n",
        "\n",
        "#TrailBlazers - Education\n",
        "# !git clone https://github.com/pankajhpatil/AlternusVera_Education.git\n",
        "\n",
        "#TrailBlazers - Event Detection\n",
        "# !git clone https://github.com/Manish0112/AlternusVera_EventDetection.git\n",
        "\n",
        "#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "# gdd.download_file_from_google_drive(file_id='1n8YWeStJV4OTfccwEli57PS9pB5SzNmv',\n",
        "#                                   dest_path='./go_ml.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Sigma - MaliciousAccount, Credibility, Network Based Predictor, Verifiable Authenticity\n",
        "# !git clone https://github.com/sacefe/cmpe257_AlternusVera_SIGMA.git\n",
        "\n",
        "\n",
        "#Cereal Killers - Social Credibility, Psychological Utility, BERT \n",
        "# !git clone https://github.com/jedvillan/CerealKillers_AlternusVera.git\n",
        "# % cd CerealKillers_AlternusVera\n",
        "# ! git fetch\n",
        "# ! git pull\n",
        "# ! git lfs pull\n",
        "# % cd /content/\n",
        "gdd.download_file_from_google_drive(file_id='1GGdgYa-V71mkTglwC9OxL0xhPU9p1hjt',\n",
        "                                  dest_path='/content/CerealKillers/BERT/saved_model/pytorch_model.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1vT5vue0V_3K1RmYuDMEkhsfrH413SKjv',\n",
        "                                  dest_path='/content/CerealKillers/PsychologicalUtility/saved_dir/net.params',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "# gdd.download_file_from_google_drive(file_id='1T04flloQktz6hOEOoboHvG2YzA_CcKTT',\n",
        "#                                   dest_path='./finalteamintegration.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1yR8bEhdPXlJdw6VLekQHWOmoDpGQre3b',\n",
        "                                  dest_path='/content/TheShinningUnicorns/SupportingFiles/Copy_of_GoogleNews-vectors-negative300.bin.gz',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "clear_output()\n",
        "print('Success clonning repos and downloding files')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success clonning repos and downloding files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8WcHhZe6sm",
        "outputId": "5d815618-0796-40ec-b8ef-806a10516eee"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "#Blastoff\n",
        "sys.path.append('/content/Blastoff/ContentStatistics')\n",
        "sys.path.append('/content/Blastoff/ContextVeracity')\n",
        "sys.path.append('/content/Blastoff/Sensationalism')\n",
        "sys.path.append('/content/Blastoff/SupportingFiles')\n",
        "\n",
        "#Cereal Killers\n",
        "sys.path.append('/content/CerealKillers/BERT')\n",
        "sys.path.append('/content/CerealKillers/PsychologicalUtility')\n",
        "sys.path.append('/content/CerealKillers/SocialCredibility')\n",
        "\n",
        "#GirlsWhoCode\n",
        "sys.path.append('/content/GirlsWhoCode/Bias')\n",
        "sys.path.append('/content/GirlsWhoCode/PoliticalAfiliation')\n",
        "sys.path.append('/content/GirlsWhoCode/Topics')\n",
        "sys.path.append('/content/GirlsWhoCode/Toxicity')\n",
        "\n",
        "#GoML\n",
        "sys.path.append('/content/GoML/ClickBait')\n",
        "sys.path.append('/content/GoML/NewsCoverage')\n",
        "sys.path.append('/content/GoML/StanceDetection')\n",
        "sys.path.append('/content/GoML/WritingStyle')\n",
        "sys.path.append('/content/GoML/SupportingFiles')\n",
        "\n",
        "#Sigma\n",
        "sys.path.append('/content/Sigma/Credibility')\n",
        "sys.path.append('/content/Sigma/MaliciousAccount')\n",
        "sys.path.append('/content/Sigma/NetworkBasedPredictor')\n",
        "sys.path.append('/content/Sigma/VerifiableAuthenticity')\n",
        "\n",
        "#Team Seekers\n",
        "sys.path.append('/content/TeamSeekers/BERTMCC')\n",
        "sys.path.append('/content/TeamSeekers/ClickBait')\n",
        "sys.path.append('/content/TeamSeekers/Spam')\n",
        "sys.path.append('/content/TeamSeekers/StanceDetection')\n",
        "\n",
        "#TheFeatureFinders\n",
        "sys.path.append('/content/TheFeatureFinders/ReliableSource')\n",
        "sys.path.append('/content/TheFeatureFinders/StanceDetection')\n",
        "sys.path.append('/content/TheFeatureFinders/Toxicity')\n",
        "sys.path.append('/content/TheFeatureFinders/SupportingFiles')\n",
        "\n",
        "#TheShinningUnicorns\n",
        "sys.path.append('/content/TheShinningUnicorns/SupportingFiles')\n",
        "\n",
        "#Trail Blaizers\n",
        "sys.path.append('/content/Trailblazers/MisleadingIntentions')\n",
        "sys.path.append('/content/Trailblazers/Education')\n",
        "sys.path.append('/content/Trailblazers/EventDetection')\n",
        "\n",
        "print('Success adding paths to the system')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success adding paths to the system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yc9Q96XNZiy",
        "outputId": "a33cf638-3c17-4a7a-bcac-e6ae48a341e8"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "import en_core_web_md\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True\n",
        "\n",
        "clear_output()\n",
        "print('Success importing dependencies')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success importing dependencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHL9hWcX17qr"
      },
      "source": [
        "#Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "033XN2S9Gd84",
        "outputId": "5fb2a679-50b7-436c-f32f-4d6af0d60744"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "print('Finish instantiating #Blastoff - Content Statistics')\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "from sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "print('Finish instantiating #Blastoff - Sensationalism')\n",
        "\n",
        "# #Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "print('Finish instantiating #Blastoff - Context Veracity')\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "print('Finish instantiating #GirlsWhoCode - Toxicity')\n",
        "\n",
        "#GirlsWhoCode - Topics\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "print('Finish instantiating #GirlsWhoCode - Topics')\n",
        "\n",
        "#GirlsWhoCode - Bias\n",
        "from bias import Gwc_Bias\n",
        "print('Finish instantiating #GirlsWhoCode - Bias')\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation\n",
        "print('Finish instantiating #GirlsWhoCode - Political Afiliation')\n",
        "\n",
        "# #Feature Finders - Stance Detection\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()\n",
        "print('Finish instantiating #Feature Finders - Stance Detection')\n",
        "\n",
        "# #Feature Finders - Toxicity\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()\n",
        "print('Finish instantiating #Feature Finders - Toxicity')\n",
        "\n",
        "# #Feature Finders - Reliable Source\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()\n",
        "print('Finish instantiating #Feature Finders - Reliable Source')\n",
        "\n",
        "# #Feature Finders - Title vs Body\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "# import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "# tvb = titlevsbody.TitleVsBody()\n",
        "# print('Finish instantiating #Feature Finders - Title vs Body')\n",
        "\n",
        "#Team Seekers - BERT\n",
        "from seekers_bertmcc import Seekers_BertMcc\n",
        "print('Finish instantiating #Team Seekers - BERT')\n",
        "\n",
        "# #Team Seekers - Stance Detection\n",
        "from seekers_stancedetection import Seekers_StanceDetection\n",
        "print('Finish instantiating #Team Seekers - Stance Detection')\n",
        "\n",
        "# #Team Seekers - ClickBait\n",
        "from seekers_clickbait import Seekers_ClickBait\n",
        "print('Finish instantiating #Team Seekers - ClickBait')\n",
        "\n",
        "# #Team Seekers - Spam\n",
        "from seekers_spam import Seekers_Spam\n",
        "print('Finish instantiating #Team Seekers - Spam')\n",
        "\n",
        "# #TrailBlazers - Misleading Intentions\n",
        "import alternusvera_misleading_intentions as MI\n",
        "print('Finish instantiating #TrailBlazers - Misleading Intentions')\n",
        "\n",
        "# #TrailBlazers - Education\n",
        "import alternusvera_education as AVE\n",
        "print('Finish instantiating #TrailBlazers - Education')\n",
        "\n",
        "# #TrailBlazers - Event Coverage\n",
        "import EventCoverage as EC\n",
        "print('Finish instantiating #TrailBlazers - Event Coverage')\n",
        "\n",
        "# #GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "# from go_ml import *\n",
        "# print('Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style')\n",
        "\n",
        "#Sigma - MaliciousAccount\n",
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/Sigma/MaliciousAccount/malicious_account_MLP.pkl\")\n",
        "print('Finish instantiating #Sigma - MaliciousAccount')\n",
        "\n",
        "# #Sigma - Credibility\n",
        "from SIGMAcredibility  import Credibility\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/Sigma/Credibility/credibility_model.pkl\")\n",
        "print('Finish instantiating #Sigma - Credibility')\n",
        "\n",
        "# #Sigma Network Based Predictor\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/Sigma/NetworkBasedPredictor/networkbased.pkl')\n",
        "print('Finish instantiating #Sigma Network Based Predictor')\n",
        "\n",
        "# #Sigma Verifiable Authenticity\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "va = VerifiableAuthenticity()\n",
        "print('Finish instantiating #Sigma Verifiable Authenticity')\n",
        "\n",
        "# #Ceral Killers - Social Credibility, Psychological Utility, BERT\n",
        "import socialcredibility as SC \n",
        "# import psychologicalutility as PU\n",
        "import bert as BT\n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "# pu = PU.CerealKillers_PsychologicalUtility()\n",
        "bt = BT.CerealKillers_SentimentClassifier()\n",
        "print('Finish instantiating #Ceral Killers - Social Credibility, Psychological Utility, BERT')\n",
        "\n",
        "# #The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "import finalteamintegration as EM\n",
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()\n",
        "print('Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics')\n",
        "\n",
        "# # clear_output()\n",
        "# # print('Success Instantiating Factors')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "spaCy not installed, install with \"python3 -m spacy download en_core_web_lg\" then restart the runtime.\n",
            "Downloading 1oTfsNgkmEBemkVfrWSjnc-K9svmL7Iak into /content/Blastoff/SupportingFiles/bcs_encoder.zip... Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-73f91b41958b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Blastoff - Content Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBlastoff_Content_Statistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlastoffContentStatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlastoffContentStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finish instantiating #Blastoff - Content Statistics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Blastoff/ContentStatistics/Blastoff_Content_Statistics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dpos, sa, ner, mpath)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/Blastoff/SupportingFiles/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Blastoff/SupportingFiles/bcs_encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   model = tf_load.load_internal(\n\u001b[0;32m--> 121\u001b[0;31m       path, options=options, loader_cls=KerasObjectLoader)\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 633\u001b[0;31m                             ckpt_options)\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models_to_reconstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasObjectLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Now that the node object has been fully loaded, and the checkpoint has\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options)\u001b[0m\n\u001b[1;32m    120\u001b[0m     self._concrete_functions = (\n\u001b[1;32m    121\u001b[0m         function_deserialization.load_function_def_library(\n\u001b[0;32m--> 122\u001b[0;31m             meta_graph.graph_def.library))\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# signatures, respectively).  function_spec is set up later by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# recreate_function(); and arg_keywords by setup_bare_concrete_function().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;31m# FuncGraph directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[0;32m-> 1542\u001b[0;31m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[1;32m   1543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    604\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[1;32m    605\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_EagerDefinedFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m   \u001b[0;34m\"\"\"Initialize the context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m           \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ContextOptionsSetTfrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_tfrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn4otJX3uTmA"
      },
      "source": [
        "!pip install autogluon==0.0.15b20201204"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0dlpFWjouvU",
        "outputId": "cc068c6f-50c8-4870-e404-b6795c04b1b1"
      },
      "source": [
        "# #Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "print('Finish instantiating #Blastoff - Context Veracity')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Finish instantiating #Blastoff - Context Veracity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1FXU_sTeb5"
      },
      "source": [
        "# Before Poli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5u_4iKQTddb"
      },
      "source": [
        "#Feature finders\n",
        "\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]\n",
        "\n",
        "# Trail Blaizers\n",
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcsZksxrHlFI"
      },
      "source": [
        "def getProbablityScore(category):\n",
        "\n",
        "    if(category =='pants-fire' ):\n",
        "        return 0.9;\n",
        "    elif(category =='false'):\n",
        "        return 0.73;\n",
        "    elif(category =='full-flop' or category == 'barely-true'):\n",
        "        return 0.56;\n",
        "    elif(category =='half-true' or category == 'half-flip'):\n",
        "        return 0.4;\n",
        "    elif(category =='mostly-true'):\n",
        "        return 0.24;\n",
        "    else:\n",
        "        return 0.1;\n",
        "\n",
        "\n",
        "def isFakeNews(headline_text, body, partyaffiliation,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    headline_text = headline_text\n",
        "    #label_encode = label_encode\n",
        "    accur = [0.57, 0.56, 0.58, 0.56,\n",
        "             0.96, 0.64, 0.76, 0.47, \n",
        "             0.64, 0.56, 0.84, \n",
        "             0.87, 0.70, 0.82, 0.85,\n",
        "             0.78, 0.82, 0.48, 0.72,\n",
        "             0.75, 0.77, 0.77, 0.70,\n",
        "             0.26, 0.28, 0.49,\n",
        "             0.82, 0.35, 0.55,\n",
        "             0.58, 0.54] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    print(w)\n",
        "\n",
        "    print(\"here\")\n",
        "    #For Toxicity - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score1 = GirlsWhoCode_Toxicity.getToxicityScore(headline_text)\n",
        "        print(\"probability of news not being fake from the toxicity model = \",score1)\n",
        "        prob.append(w[0] * score1)\n",
        "        sumW += w[0]\n",
        "\n",
        "    # #For Topic Modelling for LDA - GWC\n",
        "    # if headline_text != \"\" :\n",
        "    #     score2 = Topics_with_LDA_Bigram.getTopicScoreBigramLDAModel(headline_text)\n",
        "    #     prob.append(w[1] * score2)\n",
        "    #     print(\"probability of news not being fake from the topic model = \",score2)\n",
        "    #     sumW += w[1] \n",
        "\n",
        "    # #For Bias - GWC\n",
        "    # if headline_text != \"\":\n",
        "    #     score3 = Gwc_Bias.get_bias_score(headline_text)\n",
        "    #     print(\"probability of news not being fake from the bias model = \", score3)\n",
        "    #     prob.append(w[2] * score3)\n",
        "    #     sumW += w[2]\n",
        "\n",
        "    # #For political_affiliaiton - GWC\n",
        "    # if headline_text != \"\" and partyaffiliation != \"\":\n",
        "    #     score4 = Girlswhocode_PoliticalAfiiliation.DATAMINERS_getPartyAffiliationScore(headline_text,partyaffiliation)\n",
        "    #     print(\"probability of news not being fake political affilaiiton model = \", score4)\n",
        "    #     prob.append(w[3] * score4)\n",
        "    #     sumW += w[3]\n",
        "\n",
        "    # #For Clickbait - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "    #     prob.append(w[4] * Seekers_ClickBait().predict(headline_text))\n",
        "    #     sumW += w[4]\n",
        "    \n",
        "    # #For Stance_detection - Seekers\n",
        "    # if (headline_text != \"\"):\n",
        "\n",
        "    #     prob.append(w[5] * Seekers_StanceDetection().predict(headline_text))\n",
        "    #     sumW += w[5]\n",
        "    #For Spam - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "\n",
        "    #     prob.append(w[6] * Seekers_Spam().predict(headline_text))\n",
        "    #     sumW += w[6]\n",
        "    \n",
        "    #For Bert - Seekers\n",
        "    # if (headline_text!=\"\"):\n",
        "\n",
        "    #     prediction, probability = Seekers_BertMcc.get_bert_predictions(headline_text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "    #     prob.append(w[7] * probability)\n",
        "    #     sumW += w[7]\n",
        "\n",
        "    #For Education - Trailblazers\n",
        "    # if (headline_text != \"\"):\n",
        "        # edu_mod_res = AVE.predictLable(headline_text)\n",
        "        # edu_score=getProbablityScore(edu_mod_res)\n",
        "        # print(\"Probability of news baded on education model is \",edu_score)\n",
        "        # prob.append(w[8]* edu_score)\n",
        "        # sumW+=w[8]\n",
        "\n",
        "    #For Event Coverage - Trailblazers\n",
        "        # event_score = EC.predictLable(headline_text)\n",
        "        # print(\"Probability of news baded on Event coverage model is \",event_score[0])\n",
        "        # prob.append(w[9]* event_score[0])\n",
        "        # sumW+=w[9]\n",
        "\n",
        "    #For Misleding intentions - Trailblazers\n",
        "        # mislead_intent = (MI.predictIntention(headline_text))\n",
        "        # mislead_score=getProbablityScore(mislead_intent)\n",
        "        # print(\"Probability of news baded on misleading intension model is \",mislead_score)\n",
        "        # prob.append(w[10]* mislead_score)\n",
        "        # sumW+=w[10]\n",
        "    \n",
        "    # Stance Detection - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[11] * getStanceDetectionScore(body))\n",
        "    #     sumW += w[11]\n",
        "    \n",
        "    # Clickbait - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[12] * getClickbaitScore(body))\n",
        "    #     sumW += w[12]\n",
        "    \n",
        "    # News Coverage - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[13] * getNewsCoverageScore(body))\n",
        "    #     sumW += w[13]\n",
        "    \n",
        "    # Writing Style - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[14] * WritingStyleScore(body))\n",
        "    #     sumW += w[14]\n",
        "\n",
        "    # if (headline_text != \"\"):\n",
        "    # malicious account - Sigma\n",
        "        # factorMA, _, _  = maObject.predict(malicious_accountCls, body, nlp)\n",
        "        # prob.append(w[15] * factorMA ) \n",
        "        # sumW += w[15]\n",
        "        \n",
        "      # credebility - Sigma\n",
        "        # _ , factorCR = credObject.predict(credCls, body , nlp)\n",
        "        # prob.append(w[16] * factorCR ) \n",
        "        # sumW += w[16]\n",
        "\n",
        "      # Network base - Sigma\n",
        "        # factorNB= nb.predict(body, nlp)\n",
        "        # prob.append(w[17] * factorNB ) \n",
        "        # sumW += w[17]\n",
        "\n",
        "      # Authenticity - Sigma\n",
        "        # venue = 'CNN'\n",
        "        # factorVA = va.predict(body, venue)\n",
        "        # prob.append(w[18] * factorVA ) \n",
        "        # sumW += w[18]\n",
        "\n",
        "       # Stance Detection\n",
        "        # prob.append(w[19] * predictStance.FeatureFinders_getStanceScore(headline_text,body))\n",
        "        # sumW += w[19]\n",
        "\n",
        "      # Reliable Source\n",
        "        # prob.append(w[20] * reliablesource.FeatureFinders_getReliabilityBySource(source))\n",
        "        # sumW += w[20]\n",
        "\n",
        "       # Toxicity\n",
        "        # prob.append(w[21] * predictToxicity.FeatureFinders_getToxicityScore(headline_text,body))\n",
        "        # sumW += w[21]\n",
        "      \n",
        "       # Title vs Body\n",
        "        # tvb_value = tvb.FeatureFinders_getTitleVsBodyRelationship(head_line=headline_text, body_text=body)[0]\n",
        "        # prob.append(w[22] * tvb.FeatureFinders_getTitleVsBodyScore(tvb_value)[0][1]) # 0 is Fake, 1 is True\n",
        "        # sumW += w[22]\n",
        "\n",
        "    # Sentiment Classification\n",
        "    # if headline_text != \"\":\n",
        "    #     bt_score = bt.predict(headline_text)\n",
        "    #     prob.append(w[26]* bt_score)\n",
        "    #     sumW+=w[26]\n",
        "\n",
        "    # # # Psychological Utility\n",
        "    # if body != \"\":\n",
        "    #     pu_score = pu.predict(body,barely_true_count,false_count,half_true_count,mostly_true_count,pants_on_fire_count)\n",
        "    #     prob.append(w[27]* pu_score)\n",
        "    #     sumW+=w[27]\n",
        "      \n",
        "    # Social Credibility\n",
        "    # if source != \"\":\n",
        "    #     sc.search_user_by_name(source)\n",
        "    #     sc_score = sc.predict(sc.get_user_data())\n",
        "    #     prob.append(w[28]* sc_score)\n",
        "    #     sumW+=w[28]\n",
        "\n",
        "    # # Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #     in_df = pd.DataFrame(data=[headline_text], columns=['Statement'])\n",
        "    #     res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "    #     prob.append(w[23] * float(res_cs.iloc[0]))\n",
        "    #     sumW+=w[23]\n",
        "\n",
        "    # Context Veracity\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[24]* float(cv.get_veracity_scores(headline_text)))\n",
        "    #     sumW+=w[24]\n",
        "\n",
        "    # # Sensationalism\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[25] * 0.75 * sensa.getScore(headline_text))\n",
        "    #     sumW+=w[25]\n",
        "\n",
        "    # #For Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #      content_Statistics_score = contentStatistics1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on content Statistics model is \",content_Statistics_score)\n",
        "    #      prob.append(w[29]* content_Statistics_score)\n",
        "    #      sumW+=w[29]\n",
        "     #For Echo Chamber\n",
        "    # if (headline_text != \"\"):\n",
        "    #      echo_ChamberMaster_score = echoChamberMaster1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on echo Chamber model is \",echo_ChamberMaster_score)\n",
        "    #      prob.append(w[30]* echo_ChamberMaster_score)\n",
        "    #      sumW+=w[30]\n",
        "\n",
        "    \n",
        "   \n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpY3DW10H1y3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cad2402-d449-421f-ac2e-5a772ca90dc3"
      },
      "source": [
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes','Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.', 'democrat', 'fox news','in a report', 0,50,50,0,0)\n",
        "\n",
        "if(result>=0.8):\n",
        "    print(\"pants-on-fire\")\n",
        "elif(result<0.80 and result>=0.64):\n",
        "    print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "    print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "    print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "    print('mostly-true')\n",
        "else:\n",
        "    print('true')\n",
        "\n",
        "#if result >= 0.5:\n",
        "#     print(\"is FAKE NEWS!!!\")\n",
        "# else:\n",
        "#     print(\"it is NOT fake news!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02844311377245509, 0.02794411177644711, 0.028942115768463072, 0.02794411177644711, 0.04790419161676647, 0.03193612774451098, 0.03792415169660679, 0.023453093812375248, 0.03193612774451098, 0.02794411177644711, 0.041916167664670656, 0.04341317365269461, 0.03493013972055888, 0.04091816367265469, 0.042415169660678646, 0.038922155688622756, 0.04091816367265469, 0.023952095808383235, 0.03592814371257485, 0.037425149700598806, 0.03842315369261477, 0.03842315369261477, 0.03493013972055888, 0.012974051896207586, 0.013972055888223554, 0.02445109780439122, 0.04091816367265469, 0.01746506986027944, 0.027445109780439125, 0.028942115768463072, 0.026946107784431142]\n",
            "here\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading projection weights from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "John McCain\n",
            "Probability of news baded on education model is  0.73\n",
            "Probability of news baded on Event coverage model is  0.39483999772424655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loaded (3000000, 300) matrix from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(9 unique tokens: ['biden', 'cabinet', 'crazy', 'describ', 'fawn']...) from 1 documents (total 9 corpus positions)\n",
            "collecting document frequencies\n",
            "PROGRESS: processing document #0\n",
            "calculating IDF weights for 1 documents and 8 features (9 matrix non-zeros)\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 4000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 2 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #4 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"superhero\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"fawn\"\n",
            "topic #5 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"fawn\"\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #8 (0.100): 0.111*\"cabinet\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"describ\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"rollout\" + 0.111*\"crazy\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic diff=6.262588, rho=1.000000\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #2 (0.100): 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"cabinet\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic #0 (0.100): 0.111*\"cabinet\" + 0.111*\"rollout\" + 0.111*\"describ\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"medium\" + 0.111*\"crazy\"\n",
            "topic #6 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"crazy\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rescue\"\n",
            "topic diff=0.000186, rho=0.707018\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 8000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 4 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #6 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #1 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=6.938342, rho=1.000000\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #7 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #0 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=0.000000, rho=0.707018\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Probability of news baded on misleading intension model is  0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0e08578f6742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misFakeNews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'democrat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fox news'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in a report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pants-on-fire\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.80\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0e2c5e8ab3d6>\u001b[0m in \u001b[0;36misFakeNews\u001b[0;34m(headline_text, body, partyaffiliation, source, context, mostly_true_count, half_true_count, barely_true_count, false_count, pants_on_fire_count)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# #For Content Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m          \u001b[0mcontent_Statistics_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontentStatistics1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m          \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability of news baded on content Statistics model is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m          \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/finalteamintegration.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfTVRNjS195_"
      },
      "source": [
        "#Remove content"
      ]
    }
  ]
}