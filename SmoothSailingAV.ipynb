{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmoothSailingAV.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMAzymBGR6CPZ9yh20wbNfn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/257_Final_Exam/blob/main/SmoothSailingAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibxn0Z5m127G"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI9u-xyA-7L"
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xETwjdcu3rDo",
        "outputId": "c01d482b-ceab-4153-8ea0-7a56044809e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#UNSINTALL\n",
        "#Feature finders\n",
        "!pip uninstall -y xgboost\n",
        "clear_output()\n",
        "print('Success Uninstalling libraries')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Uninstalling libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVyOfyY3ov_"
      },
      "source": [
        "#WGET\n",
        "\n",
        "#Team Seakers\n",
        "!wget -O bertmodel https://www.dropbox.com/sh/r7hzurrs6a2x461/AABySI0lXYNT0OCwuXre_Qh1a?dl=0\n",
        "%mkdir /content/TeamSeekers/SupportingFiles\n",
        "%mv bertmodel /content/TeamSeekers/SupportingFiles/\n",
        "!unzip /content/TeamSeekers/SupportingFiles/bertmodel -d /content/TeamSeekers/SupportingFiles/\n",
        "%rm /content/TeamSeekers/SupportingFiles/bertmodel\n",
        "\n",
        "#FEATURE FINDERS\n",
        "!wget --directory-prefix=/content/TheFeatureFinders/SupportingFiles/ https://mirrors.aliyun.com/pypi/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl#sha256=898f26bb66589c644d17deff1b03961504f7ad79296ed434d0d7a5e9cb4deae6\n",
        "\n",
        "clear_output()\n",
        "print('Success Downloading files')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXWz4Ke5xF1x"
      },
      "source": [
        "# !pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5itgK72D2GDh",
        "outputId": "745fea88-f33c-4db9-d287-96705c4cea39"
      },
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#Blastoff\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "#Sigma\n",
        "!python -m spacy download en_core_web_md\n",
        "!apt-get install protobuf-compiler\n",
        "\n",
        "#Cereal Killers\n",
        "!apt-get install git-lfs\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "clear_output()\n",
        "print('Success Installing Libraries')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success Installing Libraries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a7blI8t4f7c"
      },
      "source": [
        "#RESTART RUNTIME\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szn3j9U816Bv"
      },
      "source": [
        "# Download Repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEZFScB0osBe"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from IPython.display import Image, clear_output\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3_FTsPY_bfT",
        "outputId": "2c5ff0d2-fd0e-426b-bf99-d2d5740dbc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://@github.com/roanacla/257_Final_Exam.git\n",
        "%mv 257_Final_Exam/* /content/\n",
        "%rm -r 257_Final_Exam"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '257_Final_Exam'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 212 (delta 1), reused 2 (delta 0), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (212/212), 87.36 MiB | 16.60 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "Checking out files: 100% (100/100), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAd_rjrAbTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db80379d-ce99-4446-8350-49195b36b7e6"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "# !git clone https://github.com/Wayne122/Content_Statistics_Demo.git\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "# !git clone https://github.com/roanacla/nlp_sensationalism.git\n",
        "\n",
        "#Blastoff - Context Veracity\n",
        "# !git clone https://github.com/snarvekark/Veracity_Factor.git\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "# gdd.download_file_from_google_drive(file_id='1aNCQPXSpxnTUbqkO1Jvb7c2naTpaCkJ5',\n",
        "#                                   dest_path='./toxicityclass1.py',\n",
        "#                                   unzip=False)\n",
        "#GirlsWhoCode - Topics\n",
        "# gdd.download_file_from_google_drive(file_id='1Fe9s1tzkxyI1AWqWTwhkhaZaGD34kLCz',\n",
        "#                                   dest_path='./Topics_with_LDA_Bigram.py',\n",
        "#                                   unzip=False)\n",
        "#GirlsWhoCode - Bias\n",
        "# gdd.download_file_from_google_drive(file_id='18BVbpsbJ_JIWWzdKem1KJMcq89npmBNF',\n",
        "#                                   dest_path='./bias.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "# gdd.download_file_from_google_drive(file_id='1gyQ2zdQ0OS0MWYl1-SCEX1E1CezgKuGH',\n",
        "#                                   dest_path='./GirlsWhoCode.zip',\n",
        "#                                   unzip=False)\n",
        "# with ZipFile('GirlsWhoCode.zip', 'r') as zipObj:\n",
        "#    zipObj.extractall('./GirlsWhoCode/')\n",
        "# gdd.download_file_from_google_drive(file_id='10BFSTGTiUtTTkq_FiA_ktbs0b05sUTp1',\n",
        "#                                   dest_path='./girlswhocode_political_affiliation.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Stance Detection\n",
        "# gdd.download_file_from_google_drive(file_id='1vmn6f-KE31jIZ2gANAm9bUV6BZvJ3HCC',\n",
        "#                                   dest_path='./AlternusVeraStanceDetection.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "# gdd.download_file_from_google_drive(file_id='1YmNAPeGwX3lGI_qrMIqWKTrhuEZ6wfGD',\n",
        "#                                   dest_path='./alternusveratoxicity.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Reliable Source\n",
        "# gdd.download_file_from_google_drive(file_id='14p0H4KIDNl8ICZKsC25vHvrarnlwzoiY',\n",
        "#                                   dest_path='./AlternusVeraReliableSource.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature Finders - Title vs Body\n",
        "# gdd.download_file_from_google_drive(file_id='1CyMWLWWksLLKlDkqtdSQNBJNUnwirwYS',\n",
        "#                                   dest_path='./team_features_finders_factor_title_vs_body.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Feature finders\n",
        "gdd.download_file_from_google_drive(file_id='19lFvSh2DIs9QAKQ9JbjySH4psWutny_a',\n",
        "                                  dest_path='/content/TheFeatureFinders/SupportingFiles/GoogleNews-vectors-negative300.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "# gdd.download_file_from_google_drive(file_id='1isyawf2DG4TiQw2eLtt4_c6FfjYwZL_7',\n",
        "#                                   dest_path='./titlevsbody_logistic_proba.pkl',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - BERTMCC\n",
        "# gdd.download_file_from_google_drive(file_id='104z4L5oaSJuNOo8--j3HYVBIqKLy8kzT',\n",
        "#                                   dest_path='./seekers_bertmcc.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Stance Detection\n",
        "# gdd.download_file_from_google_drive(file_id='18xB6HkP_hc8oHz2B8Gv2RctxST-dvl0l',\n",
        "#                                   dest_path='./seekers_stancedetection.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Click Bait\n",
        "# gdd.download_file_from_google_drive(file_id='1bnMx6EnB5OCFemHnoyQJg6rDWFI02BPn',\n",
        "#                                   dest_path='./seekers_clickbait.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Team Seakers - Spam\n",
        "# gdd.download_file_from_google_drive(file_id='1foTcE9fWF6aaEndSe_nNrWIPOh15jzCz',\n",
        "#                                   dest_path='./seekers_spam.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "\n",
        "#TrailBlazers - Misleading Intentions\n",
        "# !git clone https://github.com/jignesh-sjsu/AlternusVera_MisleadingIntention.git\n",
        "\n",
        "#TrailBlazers - Education\n",
        "# !git clone https://github.com/pankajhpatil/AlternusVera_Education.git\n",
        "\n",
        "#TrailBlazers - Event Detection\n",
        "# !git clone https://github.com/Manish0112/AlternusVera_EventDetection.git\n",
        "\n",
        "#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "# gdd.download_file_from_google_drive(file_id='1n8YWeStJV4OTfccwEli57PS9pB5SzNmv',\n",
        "#                                   dest_path='./go_ml.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "#Sigma - MaliciousAccount, Credibility, Network Based Predictor, Verifiable Authenticity\n",
        "# !git clone https://github.com/sacefe/cmpe257_AlternusVera_SIGMA.git\n",
        "\n",
        "\n",
        "#Cereal Killers - Social Credibility, Psychological Utility, BERT \n",
        "# !git clone https://github.com/jedvillan/CerealKillers_AlternusVera.git\n",
        "# % cd CerealKillers_AlternusVera\n",
        "# ! git fetch\n",
        "# ! git pull\n",
        "# ! git lfs pull\n",
        "# % cd /content/\n",
        "gdd.download_file_from_google_drive(file_id='1GGdgYa-V71mkTglwC9OxL0xhPU9p1hjt',\n",
        "                                  dest_path='/content/CerealKillers/BERT/saved_model/pytorch_model.bin',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1vT5vue0V_3K1RmYuDMEkhsfrH413SKjv',\n",
        "                                  dest_path='/content/CerealKillers/PsychologicalUtility/saved_dir/net.params',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "# gdd.download_file_from_google_drive(file_id='1T04flloQktz6hOEOoboHvG2YzA_CcKTT',\n",
        "#                                   dest_path='./finalteamintegration.py',\n",
        "#                                   unzip=False)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1yR8bEhdPXlJdw6VLekQHWOmoDpGQre3b',\n",
        "                                  dest_path='/content/TheShinningUnicorns/SupportingFiles/Copy of GoogleNews-vectors-negative300.bin.gz',\n",
        "                                  unzip=False,\n",
        "                                  showsize=True)\n",
        "\n",
        "clear_output()\n",
        "print('Success clonning repos and downloding files')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success clonning repos and downloding files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8WcHhZe6sm",
        "outputId": "2a5e77db-df5d-4c6b-efd8-f8037337c215"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "#Blastoff\n",
        "sys.path.append('/content/Blastoff/ContentStatistics')\n",
        "sys.path.append('/content/Blastoff/ContextVeracity')\n",
        "sys.path.append('/content/Blastoff/Sensationalism')\n",
        "\n",
        "#Cereal Killers\n",
        "sys.path.append('/content/CerealKillers/BERT')\n",
        "sys.path.append('/content/CerealKillers/PsychologicalUtility')\n",
        "sys.path.append('/content/CerealKillers/SocialCredibility')\n",
        "\n",
        "#GirlsWhoCode\n",
        "sys.path.append('/content/GirlsWhoCode/Bias')\n",
        "sys.path.append('/content/GirlsWhoCode/PoliticalAfiliation')\n",
        "sys.path.append('/content/GirlsWhoCode/Topics')\n",
        "sys.path.append('/content/GirlsWhoCode/Toxicity')\n",
        "\n",
        "#GoML\n",
        "sys.path.append('/content/GoML/ClickBait')\n",
        "sys.path.append('/content/GoML/NewsCoverage')\n",
        "sys.path.append('/content/GoML/StanceDetection')\n",
        "sys.path.append('/content/GoML/WritingStyle')\n",
        "sys.path.append('/content/GoML/SupportingFiles')\n",
        "\n",
        "#Sigma\n",
        "sys.path.append('/content/Sigma/Credibility')\n",
        "sys.path.append('/content/Sigma/MaliciousAccount')\n",
        "sys.path.append('/content/Sigma/NetworkBasedPredictor')\n",
        "sys.path.append('/content/Sigma/VerifiableAuthenticity')\n",
        "\n",
        "#Team Seekers\n",
        "sys.path.append('/content/TeamSeekers/BERTMCC')\n",
        "sys.path.append('/content/TeamSeekers/ClickBait')\n",
        "sys.path.append('/content/TeamSeekers/Spam')\n",
        "sys.path.append('/content/TeamSeekers/StanceDetection')\n",
        "\n",
        "#TheFeatureFinders\n",
        "sys.path.append('/content/TheFeatureFinders/ReliableSource')\n",
        "sys.path.append('/content/TheFeatureFinders/StanceDetection')\n",
        "sys.path.append('/content/TheFeatureFinders/Toxicity')\n",
        "sys.path.append('/content/TheFeatureFinders/SupportingFiles')\n",
        "\n",
        "#TheShinningUnicorns\n",
        "sys.path.append('/content/TheShinningUnicorns/SupportingFiles')\n",
        "\n",
        "#Trail Blaizers\n",
        "sys.path.append('/content/AlternusVera_MisleadingIntention')\n",
        "sys.path.append('/content/AlternusVera_Education')\n",
        "sys.path.append('/content/AlternusVera_EventDetection')\n",
        "\n",
        "\n",
        "\n",
        "print('Success adding paths to the system')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success adding paths to the system\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yc9Q96XNZiy",
        "outputId": "7ba64a0e-070f-4c67-ca25-2f41a7ccf922"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import math\n",
        "import pickle\n",
        "import time\n",
        "import spacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span\n",
        "import en_core_web_md\n",
        "from zipfile import ZipFile\n",
        "\n",
        "nlp = en_core_web_md.load()\n",
        "Loaded_en_core = True\n",
        "\n",
        "clear_output()\n",
        "print('Success importing dependencies')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success importing dependencies\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHL9hWcX17qr"
      },
      "source": [
        "#Instantiate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "033XN2S9Gd84",
        "outputId": "3ecdb933-c59c-4949-d4ee-d613794518b0"
      },
      "source": [
        "#Blastoff - Content Statistics\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "bcs = BlastoffContentStatistics()\n",
        "print('Finish instantiating #Blastoff - Content Statistics')\n",
        "\n",
        "#Blastoff - Sensationalism\n",
        "from sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "print('Finish instantiating #Blastoff - Sensationalism')\n",
        "\n",
        "#Blastoff - Context Veracity\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "print('Finish instantiating #Blastoff - Context Veracity')\n",
        "\n",
        "#GirlsWhoCode - Toxicity\n",
        "from toxicityclass1 import GirlsWhoCode_Toxicity\n",
        "print('Finish instantiating #GirlsWhoCode - Toxicity')\n",
        "\n",
        "#GirlsWhoCode - Topics\n",
        "from Topics_with_LDA_Bigram import Topics_with_LDA_Bigram\n",
        "print('Finish instantiating #GirlsWhoCode - Topics')\n",
        "\n",
        "#GirlsWhoCode - Bias\n",
        "from bias import Gwc_Bias\n",
        "print('Finish instantiating #GirlsWhoCode - Bias')\n",
        "\n",
        "#GirlsWhoCode - Political Afiliation\n",
        "from girlswhocode_political_affiliation import Girlswhocode_PoliticalAfiiliation\n",
        "print('Finish instantiating #GirlsWhoCode - Political Afiliation')\n",
        "\n",
        "#Feature Finders - Stance Detection\n",
        "import AlternusVeraStanceDetection as sp\n",
        "predictStance = sp.StanceDitectionFeature()\n",
        "print('Finish instantiating #Feature Finders - Stance Detection')\n",
        "\n",
        "#Feature Finders - Toxicity\n",
        "import alternusveratoxicity as tx\n",
        "predictToxicity = tx.ToxicityFeature()\n",
        "print('Finish instantiating #Feature Finders - Toxicity')\n",
        "\n",
        "#Feature Finders - Reliable Source\n",
        "import AlternusVeraReliableSource as rs\n",
        "reliablesource = rs.ReliableSource()\n",
        "print('Finish instantiating #Feature Finders - Reliable Source')\n",
        "\n",
        "#Feature Finders - Title vs Body\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "import team_features_finders_factor_title_vs_body as titlevsbody\n",
        "tvb = titlevsbody.TitleVsBody()\n",
        "print('Finish instantiating #Feature Finders - Title vs Body')\n",
        "\n",
        "#Team Seekers - BERT\n",
        "from seekers_bertmcc import Seekers_BertMcc\n",
        "print('Finish instantiating #Team Seekers - BERT')\n",
        "\n",
        "#Team Seekers - Stance Detection\n",
        "from seekers_stancedetection import Seekers_StanceDetection\n",
        "print('Finish instantiating #Team Seekers - Stance Detection')\n",
        "\n",
        "#Team Seekers - ClickBait\n",
        "from seekers_clickbait import Seekers_ClickBait\n",
        "print('Finish instantiating #Team Seekers - ClickBait')\n",
        "\n",
        "#Team Seekers - Spam\n",
        "from seekers_spam import Seekers_Spam\n",
        "print('Finish instantiating #Team Seekers - Spam')\n",
        "\n",
        "#TrailBlazers - Misleading Intentions\n",
        "import alternusvera_misleading_intentions as MI\n",
        "print('Finish instantiating #TrailBlazers - Misleading Intentions')\n",
        "\n",
        "#TrailBlazers - Education\n",
        "import alternusvera_education as AVE\n",
        "print('Finish instantiating #TrailBlazers - Education')\n",
        "\n",
        "#TrailBlazers - Event Coverage\n",
        "import EventCoverage as EC\n",
        "print('Finish instantiating #TrailBlazers - Event Coverage')\n",
        "\n",
        "#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\n",
        "from go_ml import *\n",
        "print('Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style')\n",
        "\n",
        "#Sigma - MaliciousAccount\n",
        "from SIGMAmaliciousAccount import MalicousAccount\n",
        "maObject = MalicousAccount() \n",
        "malicious_accountCls = maObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/malicious_account_MLP.pkl\")\n",
        "print('Finish instantiating #Sigma - MaliciousAccount')\n",
        "\n",
        "#Sigma - Credibility\n",
        "from SIGMAcredibility  import Credibility\n",
        "credObject = Credibility() \n",
        "credCls = credObject.load(model2load=\"/content/cmpe257_AlternusVera_SIGMA/credibility_model.pkl\")\n",
        "print('Finish instantiating #Sigma - Credibility')\n",
        "\n",
        "#Sigma Network Based Predictor\n",
        "from SIGMA_networkbased  import NetworkBasedPredictor\n",
        "nb = NetworkBasedPredictor()\n",
        "nb.load('/content/cmpe257_AlternusVera_SIGMA/networkbased.pkl')\n",
        "print('Finish instantiating #Sigma Network Based Predictor')\n",
        "\n",
        "#Sigma Verifiable Authenticity\n",
        "from SIGMAauthenticity  import VerifiableAuthenticity\n",
        "va = VerifiableAuthenticity()\n",
        "print('Finish instantiating #Sigma Verifiable Authenticity')\n",
        "\n",
        "#Ceral Killers - Social Credibility, Psychological Utility, BERT\n",
        "import socialcredibility as SC \n",
        "import psychologicalutility as PU\n",
        "import bert as BT\n",
        "sc = SC.CerealKillers_SocialCredibility()\n",
        "pu = PU.CerealKillers_PsychologicalUtility()\n",
        "bt = BT.CerealKillers_SentimentClassifier()\n",
        "print('Finish instantiating #Ceral Killers - Social Credibility, Psychological Utility, BERT')\n",
        "\n",
        "#The Shining Unicorns - Echo Chamber, Content Statistics\n",
        "import finalteamintegration as EM\n",
        "echoChamberMaster1=EM.EchoChamberMaster()\n",
        "contentStatistics1=EM.ContentStatistics()\n",
        "print('Finish instantiating #The Shining Unicorns - Echo Chamber, Content Statistics')\n",
        "\n",
        "# clear_output()\n",
        "# print('Success Instantiating Factors')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loading default pretrained model.\n",
            "Finish instantiating #Blastoff - Content Statistics\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Downloading 18NyBuNHikHiUzrAC4oOMUOO5KAaKouEY into ./context_veracity_models.zip... Done.\n",
            "Finish instantiating #Blastoff - Context Veracity\n",
            "Finish instantiating #GirlsWhoCode - Toxicity\n",
            "Finish instantiating #GirlsWhoCode - Topics\n",
            "Finish instantiating #GirlsWhoCode - Bias\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #GirlsWhoCode - Political Afiliation\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Stance Detection\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Toxicity\n",
            "Finish instantiating #Feature Finders - Reliable Source\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "Finish instantiating #Feature Finders - Title vs Body\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #Team Seekers - BERT\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #Team Seekers - Stance Detection\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Finish instantiating #Team Seekers - ClickBait\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Finish instantiating #Team Seekers - Spam\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Misleading Intentions\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Education\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Finish instantiating #TrailBlazers - Event Coverage\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1d1b351040c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#GoML - News Coverage, Stance Detection, ClickBait, Writing Style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgo_ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finish instantiating #GoML - News Coverage, Stance Detection, ClickBait, Writing Style'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: source code string cannot contain null bytes"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1FXU_sTeb5"
      },
      "source": [
        "# Before Poli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5u_4iKQTddb"
      },
      "source": [
        "#Feature finders\n",
        "\n",
        "# LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "def get_titleVsBodyScore(val): # input int, return between 0 and 1, being 0 = Fake,  1 = True\n",
        "    map = {'0': 1., '1': .2, '2': .8, '3': 0.}\n",
        "\n",
        "    return map[str(val)]\n",
        "\n",
        "# Trail Blaizers\n",
        "Label_map={'barely-true': 0,\n",
        "           'false' : 1,\n",
        "           'full-flop' : 2,\n",
        "           'half-flip' : 3,\n",
        "           'half-true' : 4,\n",
        "           'mostly-true' : 5,\n",
        "           'pants-fire' : 6,\n",
        "           'true' : 7}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcsZksxrHlFI"
      },
      "source": [
        "def getProbablityScore(category):\n",
        "\n",
        "    if(category =='pants-fire' ):\n",
        "        return 0.9;\n",
        "    elif(category =='false'):\n",
        "        return 0.73;\n",
        "    elif(category =='full-flop' or category == 'barely-true'):\n",
        "        return 0.56;\n",
        "    elif(category =='half-true' or category == 'half-flip'):\n",
        "        return 0.4;\n",
        "    elif(category =='mostly-true'):\n",
        "        return 0.24;\n",
        "    else:\n",
        "        return 0.1;\n",
        "\n",
        "\n",
        "def isFakeNews(headline_text, body, partyaffiliation,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count):\n",
        "    headline_text = headline_text\n",
        "    #label_encode = label_encode\n",
        "    accur = [0.57, 0.56, 0.58, 0.56,\n",
        "             0.96, 0.64, 0.76, 0.47, \n",
        "             0.64, 0.56, 0.84, \n",
        "             0.87, 0.70, 0.82, 0.85,\n",
        "             0.78, 0.82, 0.48, 0.72,\n",
        "             0.75, 0.77, 0.77, 0.70,\n",
        "             0.26, 0.28, 0.49,\n",
        "             0.82, 0.35, 0.55,\n",
        "             0.58, 0.54] # using the (normalized) accuracy as weigths\n",
        "    w = [float(i)/sum(accur) for i in accur]\n",
        "    sumW = 0\n",
        "    prob = []\n",
        "    print(w)\n",
        "\n",
        "    print(\"here\")\n",
        "    #For Toxicity - GWC\n",
        "    if headline_text != \"\" :\n",
        "        score1 = GirlsWhoCode_Toxicity.getToxicityScore(headline_text)\n",
        "        print(\"probability of news not being fake from the toxicity model = \",score1)\n",
        "        prob.append(w[0] * score1)\n",
        "        sumW += w[0]\n",
        "\n",
        "    # #For Topic Modelling for LDA - GWC\n",
        "    # if headline_text != \"\" :\n",
        "    #     score2 = Topics_with_LDA_Bigram.getTopicScoreBigramLDAModel(headline_text)\n",
        "    #     prob.append(w[1] * score2)\n",
        "    #     print(\"probability of news not being fake from the topic model = \",score2)\n",
        "    #     sumW += w[1] \n",
        "\n",
        "    # #For Bias - GWC\n",
        "    # if headline_text != \"\":\n",
        "    #     score3 = Gwc_Bias.get_bias_score(headline_text)\n",
        "    #     print(\"probability of news not being fake from the bias model = \", score3)\n",
        "    #     prob.append(w[2] * score3)\n",
        "    #     sumW += w[2]\n",
        "\n",
        "    # #For political_affiliaiton - GWC\n",
        "    # if headline_text != \"\" and partyaffiliation != \"\":\n",
        "    #     score4 = Girlswhocode_PoliticalAfiiliation.DATAMINERS_getPartyAffiliationScore(headline_text,partyaffiliation)\n",
        "    #     print(\"probability of news not being fake political affilaiiton model = \", score4)\n",
        "    #     prob.append(w[3] * score4)\n",
        "    #     sumW += w[3]\n",
        "\n",
        "    # #For Clickbait - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "    #     prob.append(w[4] * Seekers_ClickBait().predict(headline_text))\n",
        "    #     sumW += w[4]\n",
        "    \n",
        "    # #For Stance_detection - Seekers\n",
        "    # if (headline_text != \"\"):\n",
        "\n",
        "    #     prob.append(w[5] * Seekers_StanceDetection().predict(headline_text))\n",
        "    #     sumW += w[5]\n",
        "    #For Spam - Seekers\n",
        "    # if ( headline_text!=\"\"):\n",
        "\n",
        "    #     prob.append(w[6] * Seekers_Spam().predict(headline_text))\n",
        "    #     sumW += w[6]\n",
        "    \n",
        "    #For Bert - Seekers\n",
        "    # if (headline_text!=\"\"):\n",
        "\n",
        "    #     prediction, probability = Seekers_BertMcc.get_bert_predictions(headline_text,source,context,mostly_true_count,half_true_count,barely_true_count,false_count,pants_on_fire_count)\n",
        "    #     prob.append(w[7] * probability)\n",
        "    #     sumW += w[7]\n",
        "\n",
        "    #For Education - Trailblazers\n",
        "    # if (headline_text != \"\"):\n",
        "        # edu_mod_res = AVE.predictLable(headline_text)\n",
        "        # edu_score=getProbablityScore(edu_mod_res)\n",
        "        # print(\"Probability of news baded on education model is \",edu_score)\n",
        "        # prob.append(w[8]* edu_score)\n",
        "        # sumW+=w[8]\n",
        "\n",
        "    #For Event Coverage - Trailblazers\n",
        "        # event_score = EC.predictLable(headline_text)\n",
        "        # print(\"Probability of news baded on Event coverage model is \",event_score[0])\n",
        "        # prob.append(w[9]* event_score[0])\n",
        "        # sumW+=w[9]\n",
        "\n",
        "    #For Misleding intentions - Trailblazers\n",
        "        # mislead_intent = (MI.predictIntention(headline_text))\n",
        "        # mislead_score=getProbablityScore(mislead_intent)\n",
        "        # print(\"Probability of news baded on misleading intension model is \",mislead_score)\n",
        "        # prob.append(w[10]* mislead_score)\n",
        "        # sumW+=w[10]\n",
        "    \n",
        "    # Stance Detection - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[11] * getStanceDetectionScore(body))\n",
        "    #     sumW += w[11]\n",
        "    \n",
        "    # Clickbait - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[12] * getClickbaitScore(body))\n",
        "    #     sumW += w[12]\n",
        "    \n",
        "    # News Coverage - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[13] * getNewsCoverageScore(body))\n",
        "    #     sumW += w[13]\n",
        "    \n",
        "    # Writing Style - Go ML\n",
        "    # if body:\n",
        "    #     prob.append(w[14] * WritingStyleScore(body))\n",
        "    #     sumW += w[14]\n",
        "\n",
        "    # if (headline_text != \"\"):\n",
        "    # malicious account - Sigma\n",
        "        # factorMA, _, _  = maObject.predict(malicious_accountCls, body, nlp)\n",
        "        # prob.append(w[15] * factorMA ) \n",
        "        # sumW += w[15]\n",
        "        \n",
        "      # credebility - Sigma\n",
        "        # _ , factorCR = credObject.predict(credCls, body , nlp)\n",
        "        # prob.append(w[16] * factorCR ) \n",
        "        # sumW += w[16]\n",
        "\n",
        "      # Network base - Sigma\n",
        "        # factorNB= nb.predict(body, nlp)\n",
        "        # prob.append(w[17] * factorNB ) \n",
        "        # sumW += w[17]\n",
        "\n",
        "      # Authenticity - Sigma\n",
        "        # venue = 'CNN'\n",
        "        # factorVA = va.predict(body, venue)\n",
        "        # prob.append(w[18] * factorVA ) \n",
        "        # sumW += w[18]\n",
        "\n",
        "       # Stance Detection\n",
        "        # prob.append(w[19] * predictStance.FeatureFinders_getStanceScore(headline_text,body))\n",
        "        # sumW += w[19]\n",
        "\n",
        "      # Reliable Source\n",
        "        # prob.append(w[20] * reliablesource.FeatureFinders_getReliabilityBySource(source))\n",
        "        # sumW += w[20]\n",
        "\n",
        "       # Toxicity\n",
        "        # prob.append(w[21] * predictToxicity.FeatureFinders_getToxicityScore(headline_text,body))\n",
        "        # sumW += w[21]\n",
        "      \n",
        "       # Title vs Body\n",
        "        # tvb_value = tvb.FeatureFinders_getTitleVsBodyRelationship(head_line=headline_text, body_text=body)[0]\n",
        "        # prob.append(w[22] * tvb.FeatureFinders_getTitleVsBodyScore(tvb_value)[0][1]) # 0 is Fake, 1 is True\n",
        "        # sumW += w[22]\n",
        "\n",
        "    # Sentiment Classification\n",
        "    # if headline_text != \"\":\n",
        "    #     bt_score = bt.predict(headline_text)\n",
        "    #     prob.append(w[26]* bt_score)\n",
        "    #     sumW+=w[26]\n",
        "\n",
        "    # # # Psychological Utility\n",
        "    # if body != \"\":\n",
        "    #     pu_score = pu.predict(body,barely_true_count,false_count,half_true_count,mostly_true_count,pants_on_fire_count)\n",
        "    #     prob.append(w[27]* pu_score)\n",
        "    #     sumW+=w[27]\n",
        "      \n",
        "    # Social Credibility\n",
        "    # if source != \"\":\n",
        "    #     sc.search_user_by_name(source)\n",
        "    #     sc_score = sc.predict(sc.get_user_data())\n",
        "    #     prob.append(w[28]* sc_score)\n",
        "    #     sumW+=w[28]\n",
        "\n",
        "    # # Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #     in_df = pd.DataFrame(data=[headline_text], columns=['Statement'])\n",
        "    #     res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "    #     prob.append(w[23] * float(res_cs.iloc[0]))\n",
        "    #     sumW+=w[23]\n",
        "\n",
        "    # Context Veracity\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[24]* float(cv.get_veracity_scores(headline_text)))\n",
        "    #     sumW+=w[24]\n",
        "\n",
        "    # # Sensationalism\n",
        "    # if (headline_text != \"\"):\n",
        "    #     prob.append(w[25] * 0.75 * sensa.getScore(headline_text))\n",
        "    #     sumW+=w[25]\n",
        "\n",
        "    # #For Content Statistics\n",
        "    # if (headline_text != \"\"):\n",
        "    #      content_Statistics_score = contentStatistics1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on content Statistics model is \",content_Statistics_score)\n",
        "    #      prob.append(w[29]* content_Statistics_score)\n",
        "    #      sumW+=w[29]\n",
        "     #For Echo Chamber\n",
        "    # if (headline_text != \"\"):\n",
        "    #      echo_ChamberMaster_score = echoChamberMaster1.predict(headline_text)\n",
        "    #      print(\"Probability of news baded on echo Chamber model is \",echo_ChamberMaster_score)\n",
        "    #      prob.append(w[30]* echo_ChamberMaster_score)\n",
        "    #      sumW+=w[30]\n",
        "\n",
        "    \n",
        "   \n",
        "    probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "    return probTotal\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpY3DW10H1y3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cad2402-d449-421f-ac2e-5a772ca90dc3"
      },
      "source": [
        "result = isFakeNews('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes','Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.', 'democrat', 'fox news','in a report', 0,50,50,0,0)\n",
        "\n",
        "if(result>=0.8):\n",
        "    print(\"pants-on-fire\")\n",
        "elif(result<0.80 and result>=0.64):\n",
        "    print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "    print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "    print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "    print('mostly-true')\n",
        "else:\n",
        "    print('true')\n",
        "\n",
        "#if result >= 0.5:\n",
        "#     print(\"is FAKE NEWS!!!\")\n",
        "# else:\n",
        "#     print(\"it is NOT fake news!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02844311377245509, 0.02794411177644711, 0.028942115768463072, 0.02794411177644711, 0.04790419161676647, 0.03193612774451098, 0.03792415169660679, 0.023453093812375248, 0.03193612774451098, 0.02794411177644711, 0.041916167664670656, 0.04341317365269461, 0.03493013972055888, 0.04091816367265469, 0.042415169660678646, 0.038922155688622756, 0.04091816367265469, 0.023952095808383235, 0.03592814371257485, 0.037425149700598806, 0.03842315369261477, 0.03842315369261477, 0.03493013972055888, 0.012974051896207586, 0.013972055888223554, 0.02445109780439122, 0.04091816367265469, 0.01746506986027944, 0.027445109780439125, 0.028942115768463072, 0.026946107784431142]\n",
            "here\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading projection weights from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "John McCain\n",
            "Probability of news baded on education model is  0.73\n",
            "Probability of news baded on Event coverage model is  0.39483999772424655\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loaded (3000000, 300) matrix from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "adding document #0 to Dictionary(0 unique tokens: [])\n",
            "built Dictionary(9 unique tokens: ['biden', 'cabinet', 'crazy', 'describ', 'fawn']...) from 1 documents (total 9 corpus positions)\n",
            "collecting document frequencies\n",
            "PROGRESS: processing document #0\n",
            "calculating IDF weights for 1 documents and 8 features (9 matrix non-zeros)\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 4000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 2 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #4 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"superhero\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"fawn\"\n",
            "topic #5 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"fawn\"\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #8 (0.100): 0.111*\"cabinet\" + 0.111*\"superhero\" + 0.111*\"medium\" + 0.111*\"describ\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"rollout\" + 0.111*\"crazy\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic diff=6.262588, rho=1.000000\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #3 (0.100): 0.111*\"cabinet\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"fawn\"\n",
            "topic #2 (0.100): 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"crazy\" + 0.111*\"medium\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"rescue\" + 0.111*\"cabinet\"\n",
            "topic #1 (0.100): 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"rescue\" + 0.111*\"biden\" + 0.111*\"describ\" + 0.111*\"superhero\" + 0.111*\"rollout\" + 0.111*\"medium\" + 0.111*\"fawn\"\n",
            "topic #0 (0.100): 0.111*\"cabinet\" + 0.111*\"rollout\" + 0.111*\"describ\" + 0.111*\"rescue\" + 0.111*\"fawn\" + 0.111*\"superhero\" + 0.111*\"biden\" + 0.111*\"medium\" + 0.111*\"crazy\"\n",
            "topic #6 (0.100): 0.111*\"cabinet\" + 0.111*\"biden\" + 0.111*\"crazy\" + 0.111*\"rollout\" + 0.111*\"superhero\" + 0.111*\"fawn\" + 0.111*\"describ\" + 0.111*\"medium\" + 0.111*\"rescue\"\n",
            "topic diff=0.000186, rho=0.707018\n",
            "-4.163 per-word bound, 17.9 perplexity estimate based on a held-out corpus of 1 documents with 9 words\n",
            "using symmetric alpha at 0.1\n",
            "using symmetric eta at 0.1\n",
            "using serial LDA version on this node\n",
            "running online LDA training, 10 topics, 2 passes over the supplied corpus of 1 documents, updating every 8000 documents, evaluating every ~1 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "training LDA model using 4 processes\n",
            "PROGRESS: pass 0, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #6 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #1 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=6.938342, rho=1.000000\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n",
            "PROGRESS: pass 1, dispatched chunk #0 = documents up to #1/1, outstanding queue size 1\n",
            "topic #2 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #8 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #7 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #0 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic #5 (0.100): 0.111*\"biden\" + 0.111*\"cabinet\" + 0.111*\"crazy\" + 0.111*\"describ\" + 0.111*\"fawn\" + 0.111*\"medium\" + 0.111*\"rescue\" + 0.111*\"rollout\" + 0.111*\"superhero\"\n",
            "topic diff=0.000000, rho=0.707018\n",
            "nan per-word bound, nan perplexity estimate based on a held-out corpus of 1 documents with 0 words\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Probability of news baded on misleading intension model is  0.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0e08578f6742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misFakeNews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Members of the mainstream media showered President-elect Joe Biden with praise Tuesday following the rollout of his foreign policy and national security team. Biden formally introduced several Cabinet nominees and White House advisers, including Secretary of State nominee Antony Blinken, National Security Adviser pick Jake Sullivan, U.S. Ambassador to the United Nations choice Linda Thomas-Greenfield, and newly minted Special Presidential Envoy for Climate John Kerry. MEDIA OBSESSES OVER ANTONY BLINKEN\\'S GUITAR SKILLS AFTER BIDEN ANNOUNCES HIM AS SECRETARY OF STATE NOMINEEDuring an MSNBC panel discussion, PBS NewsHour White House correspondent Yamiche Alcindor appeared to agree with what she said a Democrat source had told her. I was talking to a Democrat who just said this also felt like \\'The Avengers, Alcindor said. It felt like we were being rescued from this craziness that we\\'ve all lived through from the last four years and now here are the superheroes to come and save us all.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'democrat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fox news'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in a report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pants-on-fire\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.80\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-0e2c5e8ab3d6>\u001b[0m in \u001b[0;36misFakeNews\u001b[0;34m(headline_text, body, partyaffiliation, source, context, mostly_true_count, half_true_count, barely_true_count, false_count, pants_on_fire_count)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# #For Content Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m          \u001b[0mcontent_Statistics_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontentStatistics1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m          \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability of news baded on content Statistics model is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m          \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mcontent_Statistics_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/finalteamintegration.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/MLFall2020/The-Shinning-Unicorns/Alternus_Vera_Final/Darshan_Data/finalized_model3.sav'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfTVRNjS195_"
      },
      "source": [
        "#Remove content"
      ]
    }
  ]
}