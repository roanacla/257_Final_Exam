{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlternusVera-Education.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9wuh5oYLiuFeD40xr3PpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajhpatil/AlternusVera_Education/blob/main/AlternusVera_Education.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJxbBgSx4aP_",
        "outputId": "57cedf34-7263-4421-d77c-3f3c66926397"
      },
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import re\n",
        "import statistics\n",
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import os\n",
        "\n",
        "Label_map={0: 'barely-true',\n",
        " 1: 'false',\n",
        " 2: 'full-flop',\n",
        " 3: 'half-flip',\n",
        " 4: 'half-true',\n",
        " 5: 'mostly-true',\n",
        " 6: 'pants-fire',\n",
        " 7: 'true'}\n",
        "\n",
        "speaker_map={0: 'Barack Obama',\n",
        " 1: 'Chain email',\n",
        " 2: 'Chris Christie',\n",
        " 3: 'Donald Trump',\n",
        " 4: 'Hillary Clinton',\n",
        " 5: 'John McCain',\n",
        " 6: 'Marco Rubio',\n",
        " 7: 'Mitt Romney',\n",
        " 8: 'Rick Perry',\n",
        " 9: 'Scott Walker'}\n",
        "\n",
        "def getTokerns(txt):\n",
        "  tokenized = sent_tokenize(txt)\n",
        "  for i in tokenized:\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "      wordsList = [w for w in wordsList if not w in stop_words]\n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "      counts = Counter(tag for (word, tag) in tagged)\n",
        "      total = sum(counts.values())\n",
        "      a = dict((word, float(count) / total) for (word, count) in\n",
        "              counts.items())\n",
        "      return a;\n",
        "\n",
        "def predictLable(text):\n",
        "  data = {'Statement':  [text]\n",
        "        }\n",
        "  df_test = pd.DataFrame (data, columns = ['Statement'])\n",
        "  \n",
        "  postags = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
        "\n",
        "  for i,txt in enumerate(postags):\n",
        "    df_test[txt]=0.00;\n",
        "\n",
        "  for i,txt in enumerate(df_test['Statement']):\n",
        "    a = getTokerns(txt)\n",
        "    for key in a:\n",
        "        if key in postags:\n",
        "          df_test[key][i]=a[key]\n",
        "  \n",
        "  spearker_df = df_test.filter(items=['CC', 'CD',\n",
        "       'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS',\n",
        "       'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
        "       'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
        "       'WP$', 'WRB'])\n",
        "  \n",
        "\n",
        "  cwd = os.getcwd()\n",
        "  print(cwd)\n",
        "\n",
        "  with open('./knn_speaker_Model.pkl', 'rb') as file:  \n",
        "    knn_speaker_Model = pickle.load(file)\n",
        "  \n",
        "  # print(spearker_df)\n",
        "  sp = knn_speaker_Model.predict(spearker_df)\n",
        "\n",
        "  res = sp[0,0];\n",
        "  if res - int(res) > 0.49:\n",
        "    res=int(res)+1\n",
        "  else:\n",
        "    res=int(res)\n",
        "\n",
        "  src=speaker_map.get(res)\n",
        "\n",
        "  print(src)  \n",
        "  # print(df_test)\n",
        "  for key in speaker_map:\n",
        "    if speaker_map[key] == src:\n",
        "      df_test['Source_cat']=key\n",
        "  \n",
        "  with open('./knn_truth_Model.pkl', 'rb') as file:  \n",
        "    knn_truth_Model = pickle.load(file)\n",
        "\n",
        "  source_df = df_test.filter(items=['Source_cat', 'CC', 'CD',\n",
        "       'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS',\n",
        "       'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
        "       'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
        "       'WP$', 'WRB'])\n",
        "  pred=knn_truth_Model.predict(source_df)\n",
        "\n",
        "  res_label = pred[0,0];\n",
        "  if res_label - int(res_label) > 0.49:\n",
        "    res_label=int(res_label)+1\n",
        "  else:\n",
        "    res_label=int(res_label)\n",
        "\n",
        "  # print(pred[0,0])\n",
        "  return Label_map.get(res_label)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAImqGFu4s5_",
        "outputId": "96b3ae63-a4d0-4022-c956-8a133c6b61dc"
      },
      "source": [
        "# text='Ann and I extend our congratulations to President-elect Joe Biden and Vice President-elect Kamala Harris. We know both of them as people of good will and admirable character. We pray that God may bless them in the days and years ahead.'\n",
        "\n",
        "# text='America, I’m honored that you have chosen me to lead our great country.The work ahead of us will be hard, but I promise you this: I will be a President for all Americans — whether you voted for me or not.I will keep the faith that you have placed in me.'\n",
        "# result = predictLable(text)\n",
        "# print(result)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Donald Trump\n",
            "pants-fire\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}