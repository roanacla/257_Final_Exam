{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlternusVera_Misleading_Intentions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2I82Jc7VUjz"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import nltk.sentiment\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import re\n",
        "import statistics\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from gensim import corpora, models\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "import warnings\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"stopwords\") \n",
        "\n",
        "\n",
        "def predictIntention(text):\n",
        "    def cleaning(statement):\n",
        "        import nltk\n",
        "        \n",
        "        # 1. Remove non-letters/Special Characters and Punctuations\n",
        "        news = re.sub(\"[^a-zA-Z]\", \" \", statement)\n",
        "        \n",
        "        # 2. Convert to lower case.\n",
        "        news =  news.lower()\n",
        "        \n",
        "        # 3. Tokenize.\n",
        "        news_words = nltk.word_tokenize( news)\n",
        "        \n",
        "        # 4. Convert the stopwords list to \"set\" data type.\n",
        "        stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "        \n",
        "        # 5. Remove stop words. \n",
        "        words = [w for w in  news_words  if not w in stops]\n",
        "        \n",
        "        # 6. Lemmentize \n",
        "        wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]\n",
        "        \n",
        "        # 7. Stemming\n",
        "        stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]\n",
        "        \n",
        "        # 8. Join the stemmed words back into one string separated by space, and return the result.\n",
        "        return \" \".join(stems)\n",
        "\n",
        "    def clean_spell_checker(df):\n",
        "      \n",
        "        model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "        words = model.index2word\n",
        "        w_rank = {}\n",
        "        for i,word in enumerate(words):\n",
        "            w_rank[word] = i\n",
        "\n",
        "        WORDS = w_rank\n",
        "\n",
        "        def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "        def P(word, N=sum(WORDS.values())): \n",
        "            \"Probability of `word`.\"\n",
        "            return - WORDS.get(word, 0)\n",
        "\n",
        "        def correction(word): \n",
        "            \"Most probable spelling correction for word.\"\n",
        "            return max(candidates(word), key=P)\n",
        "\n",
        "        def candidates(word): \n",
        "            \"Generate possible spelling corrections for word.\"\n",
        "            return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "        def known(words): \n",
        "            \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "            return set(w for w in words if w in WORDS)\n",
        "\n",
        "        def edits1(word):\n",
        "            \"All edits that are one edit away from `word`.\"\n",
        "            letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "            splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "            deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "            transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "            replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "            inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "            return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "        def edits2(word): \n",
        "            \"All edits that are two edits away from `word`.\"\n",
        "            return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "        def spell_checker(text):\n",
        "            all_words = re.findall(r'\\w+', text.lower()) # split sentence to words\n",
        "            spell_checked_text  = []\n",
        "            for i in range(len(all_words)):\n",
        "                spell_checked_text.append(correction(all_words[i]))\n",
        "            return ' '.join(spell_checked_text)\n",
        "\n",
        "        df['clean'] = df['clean'].apply(spell_checker)\n",
        "\n",
        "        return df\n",
        "        \n",
        "    cleaned_word = []\n",
        "    def clean(df):\n",
        "        df['clean'] = df['clean'].apply(cleaning)\n",
        "        df = clean_spell_checker(df) \n",
        "        return df\n",
        "\n",
        "\n",
        "    sentiment_vector = []\n",
        "    vader_polarity = []\n",
        "    sentiment_score = []\n",
        "    def sentiment_analysis(df):\n",
        "        senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
        "\n",
        "        def print_sentiment_scores(sentence):\n",
        "            snt = senti.polarity_scores(sentence)\n",
        "            # print(\"{:-<40} \\n{}\".format(sentence, str(snt)))\n",
        "            \n",
        "        print_sentiment_scores(df['clean'][0])\n",
        "\n",
        "\n",
        "        def get_vader_polarity(snt):\n",
        "            if not snt:\n",
        "                return None\n",
        "            elif snt['neg'] > snt['pos'] and snt['neg'] > snt['neu']:\n",
        "                return -1\n",
        "            elif snt['pos'] > snt['neg'] and snt['pos'] > snt['neu']:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "\n",
        "        def get_polarity_type(sentence):\n",
        "            sentimentVector = []\n",
        "            snt = senti.polarity_scores(sentence)\n",
        "            sentimentVector.append(get_vader_polarity(snt))\n",
        "            sentimentVector.append(snt['neg'])\n",
        "            sentimentVector.append(snt['neu'])\n",
        "            sentimentVector.append(snt['pos'])\n",
        "            sentimentVector.append(snt['compound'])\n",
        "            \n",
        "            return sentimentVector\n",
        "\n",
        "        get_pols = get_polarity_type(text)\n",
        "            \n",
        "        sentiment_vector = get_pols[1:]\n",
        "        vader_polarity = get_pols[0]\n",
        "        neg_score = get_pols[1]\n",
        "        neu_score = get_pols[2]\n",
        "        pos_score = get_pols[3]\n",
        "        sentiment_score = get_pols[1:][-1]\n",
        "        \n",
        "        df['sentiment_score'] = sentiment_score\n",
        "        df['vader_polarity'] = vader_polarity\n",
        "        return df\n",
        "\n",
        "    def get_sensational_score(df):\n",
        "        # sensational_words = pd.read_csv('./sensational_words_dict.csv', usecols=[0], sep='\\t+', header=None)\n",
        "        sensational_words = pd.read_csv('./AlternusVera_MisleadingIntention/Datasets/sensational_words_dict.csv', usecols=[0], sep='\\t+', header=None)\n",
        "        corpus = []\n",
        "        corpus.append(text)\n",
        "        sensational_corpus=[]\n",
        "        sensational_dictionary = ' '.join(sensational_words[0].astype(str))\n",
        "        sensational_corpus.append(sensational_dictionary)\n",
        "        \n",
        "        # sentic_net = pd.read_csv('./senticnet5.txt', sep=\"\\t+\", header=None, usecols=[0,1,2], names = [\"Token\", \"Polarity\", \"Intensity\"])\n",
        "        sentic_net = pd.read_csv('./AlternusVera_MisleadingIntention/Datasets/senticnet5.txt', sep=\"\\t+\", header=None, usecols=[0,1,2], names = [\"Token\", \"Polarity\", \"Intensity\"])\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        sentic_net = sentic_net[~sentic_net['Token'].str.contains('|'.join('_'),na=False)]\n",
        "        sentic_net = sentic_net.reset_index(drop=True)\n",
        "        senti_pos = sentic_net.loc[sentic_net.Polarity == \"positive\"]\n",
        "        senti_pos = senti_pos.loc[senti_pos.Intensity > 0.90]\n",
        "        dictionary = ' '.join(senti_pos.Token.astype(str))\n",
        "        sensational_corpus.append(dictionary)\n",
        "        \n",
        "        tfidfVec = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "        train_tfidf = tfidfVec.fit_transform(df['clean'])\n",
        "        max_f = train_tfidf.shape[1]\n",
        "        \n",
        "        tfidfVec = TfidfVectorizer(max_features=max_f)\n",
        "        tfidf_corpus = tfidfVec.fit_transform(corpus)\n",
        "        tf_idf_senti = tfidfVec.fit_transform(sensational_corpus)\n",
        "        words = tfidfVec.get_feature_names()\n",
        "        \n",
        "        tfidf_corpus.toarray()\n",
        "\n",
        "        tf_idf_senti.toarray()\n",
        "\n",
        "        tfidfVec.vocabulary_\n",
        "        \n",
        "        similarity_score = []\n",
        "        for i in range(len(train_tfidf.toarray())):\n",
        "            similarity_score.append(1 - spatial.distance.cosine(tf_idf_senti[0].toarray(), tfidf_corpus[i].toarray()))\n",
        "\n",
        "        df['sensational_score'] = similarity_score[0]\n",
        "        return df\n",
        "\n",
        "\n",
        "    def get_lda_score(df):\n",
        "        data = df\n",
        "        train_lda = data[['clean','index']]\n",
        "        processed_docs = train_lda['clean'].map(lambda doc: doc.split(\" \"))\n",
        "        # print(processed_docs)\n",
        "        dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "        # print(dictionary)\n",
        "        # dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=100000)\n",
        "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "        # print(bow_corpus)\n",
        "        tfidf = models.TfidfModel(bow_corpus)\n",
        "        corpus_tfidf = tfidf[bow_corpus]\n",
        "        lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
        "        lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "        for i, data in enumerate(bow_corpus):\n",
        "            for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
        "                df['lda_score'] = score\n",
        "\n",
        "        return df\n",
        "\n",
        "    \n",
        "    def get_POS(df):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        postags = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']\n",
        "\n",
        "        for i,txt in enumerate(postags):\n",
        "          df[txt]=0.00\n",
        "\n",
        "        def getTokerns(txt):\n",
        "          tokenized = sent_tokenize(txt)\n",
        "          for i in tokenized:\n",
        "              wordsList = nltk.word_tokenize(i)\n",
        "              wordsList = [w for w in wordsList if not w in stop_words]\n",
        "              tagged = nltk.pos_tag(wordsList)\n",
        "              counts = Counter(tag for (word, tag) in tagged)\n",
        "              total = sum(counts.values())\n",
        "              a = dict((word, float(count) / total) for (word, count) in\n",
        "                      counts.items())\n",
        "              return a;\n",
        "\n",
        "        for i,txt in enumerate(df['clean']):\n",
        "          a = getTokerns(txt)\n",
        "          for key in a:\n",
        "                if key in postags:\n",
        "                   df[key][i]=a[key]\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    df_data = pd.DataFrame([[text,0]],columns=['clean', 'index'])\n",
        "    df_data = clean(df_data)\n",
        "    df_data = sentiment_analysis(df_data)\n",
        "    df_data = get_sensational_score(df_data)\n",
        "    df_data = get_lda_score(df_data)\n",
        "    df_data = get_POS(df_data)\n",
        "    \n",
        "    df = df_data.filter(items=['lda_score','sensational_score','sentiment_score','vader_polarity',\n",
        "                               'CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS',\n",
        "                               'NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM',\n",
        "                               'TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB'])\n",
        "       \n",
        "    with open('./AlternusVera_MisleadingIntention/neural_net.pkl', 'rb') as file:  \n",
        "    # with open('./neural_net.pkl', 'rb') as file:  \n",
        "        neural_net_model = pickle.load(file)\n",
        "\n",
        "    pred = neural_net_model.predict(df)\n",
        "    # print(pred[0])\n",
        "    \n",
        "    MI_Label_map={0:'pants-fire',\n",
        "                  1:'false',\n",
        "                  2:'barely-true',\n",
        "                  3:'half-true',\n",
        "                  4:'mostly-true',\n",
        "                  5:'true'}\n",
        "\n",
        "    return MI_Label_map.get(pred[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}